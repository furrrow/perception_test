{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepmind/perception_test/blob/main/data_visualisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Up7CEnJriAVu"
      },
      "outputs": [],
      "source": [
        "#@title Prerequisites\n",
        "import sys\n",
        "import os\n",
        "import io\n",
        "import json\n",
        "import random\n",
        "import imageio\n",
        "import colorsys\n",
        "import cv2\n",
        "import numpy as np\n",
        "import moviepy.editor as mvp\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "\n",
        "from IPython.display import Audio\n",
        "from google.colab.patches import cv2_imshow\n",
        "from scipy.io import wavfile\n",
        "from typing import Tuple, List"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download Dataset Sample\n",
        "!mkdir data\n",
        "!wget https://storage.googleapis.com/dm-perception-test/zip_data/sample_annotations.zip\n",
        "!unzip sample_annotations.zip -d data/annotations\n",
        "!rm sample_annotations.zip\n",
        "\n",
        "!wget https://storage.googleapis.com/dm-perception-test/zip_data/sample_videos.zip\n",
        "!unzip sample_videos.zip -d data/\n",
        "!rm sample_videos.zip"
      ],
      "metadata": {
        "id": "fymsuk6HbPsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Utility Functions\n",
        "def load_db_json(db_path: str) -> dict:\n",
        "    \"\"\"\n",
        "    Loads a JSON file as a dictionary.\n",
        "\n",
        "    Args:\n",
        "        db_path (str): Path to the JSON file.\n",
        "\n",
        "    Returns:\n",
        "        dict: Loaded JSON data as a dictionary.\n",
        "    \"\"\"\n",
        "    if not os.path.isfile(db_path):\n",
        "        raise FileNotFoundError(f\"No such file: '{db_path}'\")\n",
        "\n",
        "    with open(db_path, 'r') as f:\n",
        "        label_dict = json.load(f)\n",
        "        if not isinstance(label_dict, dict):\n",
        "            raise TypeError(\"JSON file is not formatted as a dictionary.\")\n",
        "        return label_dict\n",
        "\n",
        "\n",
        "def load_mp4_to_frames(filename: str) -> np.array:\n",
        "    \"\"\"\n",
        "    Loads an MP4 video file and returns its frames as a NumPy array.\n",
        "\n",
        "    Args:\n",
        "        filename (str): Path to the MP4 video file.\n",
        "\n",
        "    Returns:\n",
        "        np.array: Frames of the video as a NumPy array.\n",
        "    \"\"\"\n",
        "    assert os.path.exists(filename), f\"File '{filename}' does not exist.\"\n",
        "    cap = cv2.VideoCapture(filename)\n",
        "\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "\n",
        "    frames = np.empty((num_frames, height, width, 3), dtype=np.uint8)\n",
        "\n",
        "    idx = 0\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frames[idx] = frame\n",
        "        idx += 1\n",
        "\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "\n",
        "def get_video_frames(video_item: dict, video_path: str) -> np.array:\n",
        "    \"\"\"\n",
        "    Loads frames of a video specified by an item from the dataset.\n",
        "\n",
        "    Args:\n",
        "        video_item (dict): Item from dataset containing metadata.\n",
        "        video_path (str): Path to the directory containing videos.\n",
        "\n",
        "    Returns:\n",
        "        np.array: Frames of the video as a NumPy array.\n",
        "    \"\"\"\n",
        "    video_file_path = os.path.join(video_path,\n",
        "                                video_item['metadata']['video_id']) + '.mp4'\n",
        "    frames = load_mp4_to_frames(video_file_path)\n",
        "    assert video_item['metadata']['num_frames'] == frames.shape[0], \\\n",
        "        print(video_item['metadata']['num_frames'], frames.shape[0])\n",
        "    return frames\n",
        "\n",
        "\n",
        "def get_audio(audio_item: dict, audio_path: str) -> np.array:\n",
        "    \"\"\"\n",
        "    Loads audio specified by an item from the dataset.\n",
        "\n",
        "    Args:\n",
        "        audio_item (dict): Item from dataset containing metadata.\n",
        "        audio_path (str): Path to the directory containing audios.\n",
        "\n",
        "    Returns:\n",
        "        np.array: Audio as a NumPy array.\n",
        "    \"\"\"\n",
        "    audio_file_path = os.path.join(audio_path,\n",
        "                                audio_item['metadata']['video_id']) + '.wav'\n",
        "    sample_rate, audio = wavfile.read(audio_file_path)\n",
        "\n",
        "    assert audio_item['metadata']['audio_samples'] == audio.shape[0]\n",
        "    assert audio_item['metadata']['audio_sample_rate'] == sample_rate\n",
        "\n",
        "    return audio.astype(np.float32)"
      ],
      "metadata": {
        "id": "1q_JZ0ueikxp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualisation functions\n",
        "def get_colors(num_colors: int) -> Tuple[int, int, int]:\n",
        "    \"\"\"\n",
        "    Generate random colormaps for visualizing different objects and points.\n",
        "\n",
        "    Parameters:\n",
        "    - num_colors (int): The number of colors to generate.\n",
        "\n",
        "    Returns:\n",
        "    - Tuple[int, int, int]: A tuple of RGB values representing the\n",
        "    generated colors.\n",
        "    \"\"\"\n",
        "    colors = []\n",
        "    for i in np.arange(0., 360., 360. / num_colors):\n",
        "        hue = i / 360.\n",
        "        lightness = (50 + np.random.rand() * 10) / 100.\n",
        "        saturation = (90 + np.random.rand() * 10) / 100.\n",
        "        color = colorsys.hls_to_rgb(hue, lightness, saturation)\n",
        "        color = (int(color[0] * 255), int(color[1] * 255), int(color[2] * 255))\n",
        "        colors.append(color)\n",
        "    random.seed(0)\n",
        "    random.shuffle(colors)\n",
        "    return colors\n",
        "\n",
        "\n",
        "def display_video(video: np.array, fps: int=30):\n",
        "    \"\"\"\n",
        "    Create and display temporary video from numpy array frames.\n",
        "\n",
        "    Parameters:\n",
        "    - video (np.array): The input video frames.\n",
        "    format of frames should be: (num_frames, height, width, channels)\n",
        "    - fps (int): Frames per second for the video playback. Default is 30.\n",
        "    \"\"\"\n",
        "    kargs = { 'macro_block_size': None } # to avoid auto resizing\n",
        "    imageio.mimwrite('tmp_video_display.mp4',\n",
        "                     video[:, :, :, ::-1], fps=fps, **kargs)\n",
        "    display(mvp.ipython_display('tmp_video_display.mp4'))\n",
        "\n",
        "\n",
        "def display_frame(frame: np.array):\n",
        "    \"\"\"\n",
        "    Display a frame.\n",
        "\n",
        "    Parameters:\n",
        "    - frame (np.array): The frame to be displayed.\n",
        "    \"\"\"\n",
        "    cv2_imshow(frame)\n",
        "\n",
        "def paint_box(video: np.array, track: dict, color=(255, 0, 0)) -> np.array:\n",
        "    \"\"\"\n",
        "    Paint bounding box and label on frames of a video for a given track.\n",
        "\n",
        "    Parameters:\n",
        "    - video (np.array): The input video frames.\n",
        "    - track (dict): The track information containing bounding box and frame information.\n",
        "    - color (Tuple[int, int, int], optional): The RGB color values for the\n",
        "    bounding box. Default is red (255, 0, 0).\n",
        "\n",
        "    Returns:\n",
        "    - np.array: The modified video frames with painted bounding box and label.\n",
        "    \"\"\"\n",
        "    num_frames, height, width, _ = video.shape\n",
        "    name = str(track['id']) + ' : ' + track['label']\n",
        "    bounding_boxes = np.array(track['bounding_boxes']).T\n",
        "\n",
        "    for box, frame_idx in zip(bounding_boxes, track['frame_ids']):\n",
        "        frame = np.array(video[frame_idx])\n",
        "        y1 = int(round(box[2] * height))\n",
        "        x1 = int(round(box[3] * width))\n",
        "        y2 = int(round(box[0] * height))\n",
        "        x2 = int(round(box[1] * width))\n",
        "        frame = cv2.rectangle(frame, (x1, y1), (x2, y2),\n",
        "                              color=color, thickness=2)\n",
        "        frame = cv2.putText(frame, name, (x1, y1 + 20),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.75, color, 2)\n",
        "        video[frame_idx] = frame\n",
        "\n",
        "    return video\n",
        "\n",
        "def paint_boxes(video: np.array, tracks: List[dict]) -> np.array:\n",
        "    \"\"\"\n",
        "    Paint bounding boxes and labels on frames of a video for multiple tracks.\n",
        "\n",
        "    Parameters:\n",
        "    - video (np.array): The input video frames.\n",
        "    - tracks (List[dict]): A list of track information,\n",
        "    where each track contains bounding box and frame information.\n",
        "\n",
        "    Returns:\n",
        "    - np.array: The modified video frames with painted bounding boxes\n",
        "    and labels.\n",
        "    \"\"\"\n",
        "    for i, track in enumerate(tracks):\n",
        "        video = paint_box(video, track, COLORS[i])\n",
        "    return video\n",
        "\n",
        "\n",
        "def paint_point(video: np.array,\n",
        "                track: dict, color: tuple = (255, 0, 0)) -> np.array:\n",
        "    \"\"\"\n",
        "    Paints a single tracked point on each frame of a video.\n",
        "\n",
        "    Args:\n",
        "        video (np.array): The input video frames.\n",
        "        track (dict): The track containing frame IDs and corresponding points.\n",
        "        color (tuple, optional): The color of the painted point.\n",
        "        Defaults to (255, 0, 0).\n",
        "\n",
        "    Returns:\n",
        "        np.array: The video frames with painted points.\n",
        "    \"\"\"\n",
        "    num_frames, height, width, _ = video.shape\n",
        "    for idx, frame_id in enumerate(track['frame_ids']):\n",
        "        frame = video[frame_id]\n",
        "        y = int(round(track['points'][0][idx] * height))\n",
        "        x = int(round(track['points'][1][idx] * width))\n",
        "        frame = cv2.circle(frame, (x, y), radius=10, color=color, thickness=-1)\n",
        "        video[frame_id] = frame\n",
        "    return video\n",
        "\n",
        "\n",
        "def paint_points(video: np.array, tracks: List[dict]) -> np.array:\n",
        "    \"\"\"\n",
        "    Paints multiple tracked points on each frame of a video.\n",
        "\n",
        "    Args:\n",
        "        video (np.array): The input video frames.\n",
        "        tracks (List[dict]): The list of tracks containing\n",
        "        frame IDs and corresponding points.\n",
        "\n",
        "    Returns:\n",
        "        np.array: The video frames with painted points.\n",
        "    \"\"\"\n",
        "    for i, track in enumerate(tracks):\n",
        "        video = paint_point(video, track, COLORS[i])\n",
        "    return video\n",
        "\n",
        "\n",
        "def paint_action(video: np.array, action: dict,\n",
        "                 labelled_frames: np.array,\n",
        "                 color: tuple = (0, 255, 0)) -> np.array:\n",
        "    \"\"\"\n",
        "    Paints an action label on each frame of a video.\n",
        "\n",
        "    Args:\n",
        "        video (np.array): The input video frames.\n",
        "        action (dict): The action containing the label and frame IDs.\n",
        "        labelled_frames (np.array): The array to keep track\n",
        "        of the number of labels on each frame.\n",
        "        color (tuple, optional): The color of the painted label.\n",
        "        Defaults to (0, 255, 0).\n",
        "\n",
        "    Returns:\n",
        "        np.array: The video frames with painted labels.\n",
        "    \"\"\"\n",
        "    num_frames, height, width, _ = video.shape\n",
        "    name = 'Action: ' + action['label']\n",
        "    [start_frame, end_frame] = action['frame_ids']\n",
        "\n",
        "    for frame_idx in range(start_frame, end_frame):\n",
        "        frame = np.array(video[frame_idx])\n",
        "        y1 = int(round(0.9 * height) - (40 * labelled_frames[frame_idx]))\n",
        "        x1 = int(round(0.05 * width))\n",
        "\n",
        "        frame = cv2.putText(frame, name, (x1, y1),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 1.00, color, 2)\n",
        "        video[frame_idx] = frame\n",
        "        labelled_frames[frame_idx] += 1\n",
        "\n",
        "    return video\n",
        "\n",
        "\n",
        "def paint_actions(video: np.array,\n",
        "                  actions: List[dict], labelled_frames: np.array) -> np.array:\n",
        "    \"\"\"\n",
        "    Paints multiple action labels on each frame of a video.\n",
        "\n",
        "    Args:\n",
        "        video (np.array): The input video frames.\n",
        "        actions (List[dict]): The list of actions containing\n",
        "        the labels and frame IDs.\n",
        "        labelled_frames (np.array): The array to keep track\n",
        "        of the number of labels on each frame.\n",
        "\n",
        "    Returns:\n",
        "        np.array: The video frames with painted labels.\n",
        "    \"\"\"\n",
        "    for i, action in enumerate(actions):\n",
        "        video = paint_action(video, action, labelled_frames)\n",
        "    return video\n",
        "\n",
        "\n",
        "def paint_sound(video: np.array,\n",
        "                sound: dict, labelled_frames: np.array,\n",
        "                color: tuple = (0, 0, 255)) -> np.array:\n",
        "    \"\"\"\n",
        "    Paints a sound label on each frame of a video.\n",
        "\n",
        "    Args:\n",
        "        video (np.array): The input video frames.\n",
        "        sound (dict): The sound containing the label,\n",
        "        frame IDs, and visibility.\n",
        "        labelled_frames (np.array): The array to keep track of\n",
        "        the number of labels on each frame.\n",
        "        color (tuple, optional): The color of the painted label.\n",
        "        Defaults to (0, 0, 255).\n",
        "\n",
        "    Returns:\n",
        "        np.array: The video frames with painted labels.\n",
        "    \"\"\"\n",
        "    num_frames, height, width, _ = video.shape\n",
        "    name = 'Sound: ' + sound['label'] + ' is_visible: ' + str(bool(sound['is_visible']))\n",
        "    [start_frame, end_frame] = sound['frame_ids']\n",
        "\n",
        "    for frame_idx in range(start_frame, end_frame):\n",
        "        frame = np.array(video[frame_idx])\n",
        "        y1 = int(round(0.9 * height) - (40 * labelled_frames[frame_idx]))\n",
        "        x1 = int(round(0.6 * width))\n",
        "\n",
        "        frame = cv2.putText(frame, name, (x1, y1),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 1.0, color, 2)\n",
        "        video[frame_idx] = frame\n",
        "        labelled_frames[frame_idx] += 1\n",
        "\n",
        "    return video\n",
        "\n",
        "\n",
        "def paint_sounds(video: np.array,\n",
        "                 sounds: List[dict], labelled_frames: np.array) -> np.array:\n",
        "    \"\"\"\n",
        "    Paints multiple sound labels on each frame of a video.\n",
        "\n",
        "    Args:\n",
        "        video (np.array): The input video frames.\n",
        "        sounds (List[dict]): The list of sounds containing the labels,\n",
        "        frame IDs, and visibility.\n",
        "        labelled_frames (np.array): The array to keep track of the\n",
        "        number of labels on each frame.\n",
        "\n",
        "    Returns:\n",
        "        np.array: The video frames with painted labels.\n",
        "    \"\"\"\n",
        "    for i, sound in enumerate(sounds):\n",
        "        video = paint_sound(video, sound, labelled_frames)\n",
        "    return video\n",
        "\n",
        "\n",
        "def get_answer_tracks(example_data: dict, goq_ids: List) -> List[dict]:\n",
        "    \"\"\"\n",
        "    Filters and retrieves object tracks from data\n",
        "    based on the given object ids.\n",
        "\n",
        "    Args:\n",
        "        example_data (dict): The data containing object tracking information.\n",
        "        goq_ids (List): The list of IDs to filter tracks.\n",
        "\n",
        "    Returns:\n",
        "        List[dict]: The filtered tracks matching the goq_ids.\n",
        "    \"\"\"\n",
        "    goq_tracks = []\n",
        "    for track in example_data['object_tracking']:\n",
        "        if track['id'] in goq_ids:\n",
        "            goq_tracks.append(track)\n",
        "    return goq_tracks"
      ],
      "metadata": {
        "id": "DhLSUAVdUGx2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Loading Annotations\n",
        "video_path = './data/videos/'\n",
        "# audio_path = './data/audios/'\n",
        "\n",
        "db_json_path = './data/annotations/sample.json'\n",
        "db_dict = load_db_json(db_json_path)"
      ],
      "metadata": {
        "id": "m3Z9Hu4Oims5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Show Example Annotations\n",
        "video_id = list(db_dict.keys())[6]\n",
        "example_data = db_dict[video_id]"
      ],
      "metadata": {
        "id": "dMxtDBPsMma2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('---------------------------------------------------------------------')\n",
        "print('Tasks annotated for this video: ')\n",
        "for k,v in example_data.items():\n",
        "    if v:\n",
        "        print(f'{k} - available: yes - annotations: {len(v)}')\n",
        "    else:\n",
        "        print(f'{k} - available: no')\n",
        "print('---------------------------------------------------------------------')\n",
        "print('Video Metadata')\n",
        "print('---------------------------------------------------------------------')\n",
        "for k,v in example_data['metadata'].items():\n",
        "    print(f'{k} : {v}')\n",
        "print('---------------------------------------------------------------------')\n",
        "print('Object Tracking data')\n",
        "print('---------------------------------------------------------------------')\n",
        "for k,v in example_data['object_tracking'][0].items():\n",
        "    print(f'{k} : {v}')\n",
        "print('---------------------------------------------------------------------')\n",
        "print('Multiple-Choice VQA')\n",
        "print('---------------------------------------------------------------------')\n",
        "for k,v in example_data['mc_question'][0].items():\n",
        "    print(f'{k} : {v}')\n",
        "print('---------------------------------------------------------------------')"
      ],
      "metadata": {
        "id": "nSbbsIDQI6Uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualising Object Tracks\n",
        "if example_data['object_tracking']:\n",
        "    frames = get_video_frames(example_data, video_path)\n",
        "\n",
        "    COLORS = get_colors(num_colors=100)\n",
        "    show_all_tracks = True  #@param {type: \"boolean\"}\n",
        "    show_track = 2  #@param {type: \"integer\"}\n",
        "\n",
        "    if show_all_tracks:\n",
        "        frames = paint_boxes(frames, example_data['object_tracking'])\n",
        "    else:\n",
        "        frames = paint_box(frames, example_data['object_tracking'][show_track])\n",
        "\n",
        "\n",
        "    annotated_frames = []\n",
        "    for frame_idx in example_data['object_tracking'][0]['frame_ids']:\n",
        "        annotated_frames.append(frames[frame_idx])\n",
        "\n",
        "    annotated_frames = np.array(annotated_frames)\n",
        "    display_video(annotated_frames, 1)\n",
        "    del frames # managing RAM in Colab"
      ],
      "metadata": {
        "id": "hPJkHKWPEvff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualising Point Tracks\n",
        "if example_data['point_tracking']:\n",
        "    frames = get_video_frames(example_data, video_path)\n",
        "    COLORS = get_colors(num_colors=100)\n",
        "    frames = paint_points(frames, example_data['point_tracking'])\n",
        "    display_video(frames, example_data['metadata']['frame_rate'])\n",
        "    del frames # managing RAM in Colab"
      ],
      "metadata": {
        "id": "cJESCRiMr9i7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualising Action & Sound Segments\n",
        "if example_data['action_localisation']:\n",
        "    frames = get_video_frames(example_data, video_path)\n",
        "    labelled_frames = np.zeros(frames.shape[0])\n",
        "    frames = paint_actions(frames, example_data['action_localisation'], labelled_frames)\n",
        "    display_video(frames, example_data['metadata']['frame_rate'])\n",
        "    del frames\n"
      ],
      "metadata": {
        "id": "_lB8XcG86YHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualising Action & Sound Segments\n",
        "if example_data['action_localisation']:\n",
        "    frames = get_video_frames(example_data, video_path)[:,:,:,::-1]\n",
        "\n",
        "    action_labels = []\n",
        "    action_start_times = []\n",
        "    action_end_times = []\n",
        "\n",
        "    for action in example_data['action_localisation']:\n",
        "        action_labels.append(action['label'])\n",
        "        action_start_times.append(action['timestamps'][0]/1e6)\n",
        "        action_end_times.append(action['timestamps'][1]/1e6)\n",
        "\n",
        "    action_start_times = np.array(action_start_times)\n",
        "    action_end_times = np.array(action_end_times)\n",
        "\n",
        "    plt.figure(figsize=(20, 15))\n",
        "    # Strip of frames\n",
        "    plt.subplot(4,1,2)\n",
        "    plt.title(\"Video Frames\")\n",
        "    f_size = frames[0].shape\n",
        "    small = tuple(reversed((np.array(f_size[:2]) / 4).astype(int)))\n",
        "    strip = None\n",
        "    num_frames = example_data['metadata']['num_frames']\n",
        "    for i in range(0, num_frames, int(num_frames/4)):\n",
        "        frame = cv2.resize(frames[i], small)\n",
        "        if strip is None:\n",
        "            strip = np.array(frame)\n",
        "        else:\n",
        "            strip = np.concatenate([strip, frame], axis=1)\n",
        "        plt.imshow(strip)\n",
        "\n",
        "    del frames\n",
        "\n",
        "    plt.subplot(4,1,3)\n",
        "    plt.title(\"Action Events\")\n",
        "    plt.barh(range(len(action_start_times)),\n",
        "            action_end_times-action_start_times,\n",
        "            left=action_start_times)\n",
        "    plt.yticks(range(len(action_start_times)), action_labels)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "NPEft_TFwcnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualising Action & Sound Segments\n",
        "if example_data['sound_localisation']:\n",
        "    frames = get_video_frames(example_data, video_path)\n",
        "    labelled_frames = np.zeros(frames.shape[0])\n",
        "    frames = paint_sounds(frames, example_data['sound_localisation'], labelled_frames)\n",
        "    display_video(frames, example_data['metadata']['frame_rate'])\n",
        "    del frames"
      ],
      "metadata": {
        "id": "NeeAYsGXuByb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualising Action & Sound Segments\n",
        "if example_data['sound_localisation']:\n",
        "    frames = get_video_frames(example_data, video_path)[:,:,:,::-1]\n",
        "\n",
        "    audio_labels = []\n",
        "    audio_start_times = []\n",
        "    audio_end_times = []\n",
        "    for audio_event in example_data['sound_localisation']:\n",
        "        audio_labels.append(audio_event['label'])\n",
        "        audio_start_times.append(audio_event['timestamps'][0]/1e6)\n",
        "        audio_end_times.append(audio_event['timestamps'][1]/1e6)\n",
        "\n",
        "    audio_start_times = np.array(audio_start_times)\n",
        "    audio_end_times = np.array(audio_end_times)\n",
        "\n",
        "    plt.figure(figsize=(20, 15))\n",
        "    # Strip of frames\n",
        "    plt.subplot(4,1,2)\n",
        "    plt.title(\"Video Frames\")\n",
        "    f_size = frames[0].shape\n",
        "    small = tuple(reversed((np.array(f_size[:2]) / 4).astype(int)))\n",
        "    strip = None\n",
        "    num_frames = example_data['metadata']['num_frames']\n",
        "    for i in range(0, num_frames, int(num_frames/4)):\n",
        "        frame = cv2.resize(frames[i], small)\n",
        "        if strip is None:\n",
        "            strip = np.array(frame)\n",
        "        else:\n",
        "            strip = np.concatenate([strip, frame], axis=1)\n",
        "        plt.imshow(strip)\n",
        "\n",
        "    del frames\n",
        "\n",
        "    # Plot audio events\n",
        "    plt.subplot(4,1,3)\n",
        "    plt.title(\"Audio Events\")\n",
        "    plt.barh(range(len(audio_start_times)),\n",
        "            audio_end_times-audio_start_times,\n",
        "            left=audio_start_times)\n",
        "    plt.yticks(range(len(audio_start_times)), audio_labels)\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "nUZW6HvWwonR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualising Multiple-Choice Video Question-Answering\n",
        "if example_data['mc_question']:\n",
        "    for question in example_data['mc_question']:\n",
        "        print('---------------------------------')\n",
        "        print('Question: ', question['question'])\n",
        "        print('Options: ', question['options'])\n",
        "        print('Correct Answer ID: ', question['answer_id'], ' - ', question['options'][question['answer_id']])\n",
        "        print('Question info: ')\n",
        "        print('Reasoning: ', question['reasoning'])\n",
        "        print('Tag: ', question['tag'])\n",
        "        print('area: ', question['area'])\n",
        "        print('---------------------------------')"
      ],
      "metadata": {
        "id": "RR9Mm_0T6iBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualising Grounded Video Question-Answering\n",
        "\n",
        "# loading and example that has grounded question annotations\n",
        "video_id = list(db_dict.keys())[7]\n",
        "example_data = db_dict[video_id]\n",
        "\n",
        "# visualising grounded question annotations\n",
        "if example_data['grounded_question']:\n",
        "    question = example_data['grounded_question'][0]\n",
        "    print('---------------------------------')\n",
        "    print('Question: ', question['question'])\n",
        "    print('Answer IDs: ', question['answers'])\n",
        "    print('Question info: ')\n",
        "    print('Reasoning: ', question['reasoning'])\n",
        "    print('area: ', question['area'])\n",
        "    print('---------------------------------')\n",
        "\n",
        "    frames = get_video_frames(example_data, video_path)\n",
        "    answer_tracks = get_answer_tracks(example_data, question['answers'])\n",
        "    frames = paint_boxes(frames, answer_tracks)\n",
        "\n",
        "    annotated_frames = []\n",
        "    for frame_idx in answer_tracks[0]['frame_ids']:\n",
        "        annotated_frames.append(frames[frame_idx])\n",
        "\n",
        "    annotated_frames = np.array(annotated_frames)\n",
        "    display_video(annotated_frames, 1)\n",
        "    del frames # managing RAM in Colab"
      ],
      "metadata": {
        "id": "3iFMOZgShAqn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
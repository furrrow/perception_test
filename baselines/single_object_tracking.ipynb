{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepmind/perception_test/blob/main/baselines/single_object_tracking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook demonstrates how to load the object tracking annotations in the validation split of the Perception Test and run the evaluation for a simple baseline model which predicts static boxes for all tracks."
      ],
      "metadata": {
        "id": "JykaDwpmWOcu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copyright 2023 DeepMind Technologies Limited\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at [https://www.apache.org/licenses/LICENSE-2.0](https://www.apache.org/licenses/LICENSE-2.0).\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n"
      ],
      "metadata": {
        "id": "Ov2zD0apsX3A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Single Object Tracking Static Baseline\n",
        "\n",
        "Github: https://github.com/deepmind/perception_test\n",
        "\n",
        "## The Perception Test\n",
        "[Perception Test: A Diagnostic Benchmark for Multimodal Video Models](https://arxiv.org/abs/2305.13786) is a multimodal benchmark designed to comprehensively evaluate the perception and reasoning skills of multimodal video models. The Perception Test dataset introduces real-world videos designed to show perceptually interesting situations and defines multiple computational tasks (object and point tracking, action and sound localisation, multiple-choice and grounded video question-answering). Here, we provide details and a simple baseline for the single object tracking task.\n",
        "\n",
        "\n",
        "##Single Object Tracking\n",
        "In the single object tracking task, the model receives a video and a bounding box representing an object, and it is required to track the object throughout the video sequence.\n",
        "\n",
        "The below image shows examples of object tracking annotations. Note that the original videos have around 30fps, but the annotations are collected at 1fps. Here we show only the annotated frames.\n",
        "\n",
        "![image collage](https://storage.googleapis.com/dm-perception-test/img/collage.png)\n",
        "\n",
        "\n",
        "## Static baseline\n",
        "This notebook demonstrates how to load the challenge subset of the annotations in the validation split of the Perception Test, and run the evaluation for a dummy baseline model. This model assumes static boxes for all object tracks in a video\n",
        "\n",
        "[![Perception Test Overview Presentation](https://img.youtube.com/vi/8BiajMOBWdk/maxresdefault.jpg)](https://youtu.be/8BiajMOBWdk?t=10)\n"
      ],
      "metadata": {
        "id": "NvbIw-q_sYm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VOT toolkit required for evaluation\n",
        "!pip install git+https://github.com/votchallenge/vot-toolkit-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlEOnxHVeoiM",
        "outputId": "13cbeca8-ae8e-4ec6-e42a-7da66e244e50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/votchallenge/vot-toolkit-python\n",
            "  Cloning https://github.com/votchallenge/vot-toolkit-python to /tmp/pip-req-build-d333lnr6\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/votchallenge/vot-toolkit-python /tmp/pip-req-build-d333lnr6\n",
            "  Resolved https://github.com/votchallenge/vot-toolkit-python to commit 41bd50230d270f52555c484c25def9ce3f650244\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: vot-trax>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from vot-toolkit==0.6.4) (4.0.1)\n",
            "Requirement already satisfied: tqdm>=4.37 in /usr/local/lib/python3.10/dist-packages (from vot-toolkit==0.6.4) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.10/dist-packages (from vot-toolkit==0.6.4) (1.22.4)\n",
            "Requirement already satisfied: opencv-python>=4.0 in /usr/local/lib/python3.10/dist-packages (from vot-toolkit==0.6.4) (4.7.0.72)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from vot-toolkit==0.6.4) (1.16.0)\n",
            "Requirement already satisfied: pylatex>=1.3 in /usr/local/lib/python3.10/dist-packages (from vot-toolkit==0.6.4) (1.4.1)\n",
            "Requirement already satisfied: jsonschema>=3.2 in /usr/local/lib/python3.10/dist-packages (from vot-toolkit==0.6.4) (4.3.3)\n",
            "Requirement already satisfied: pyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from vot-toolkit==0.6.4) (6.0)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.10/dist-packages (from vot-toolkit==0.6.4) (3.7.1)\n",
            "Requirement already satisfied: Pillow>=7.0 in /usr/local/lib/python3.10/dist-packages (from vot-toolkit==0.6.4) (8.4.0)\n",
            "Requirement already satisfied: numba>=0.47 in /usr/local/lib/python3.10/dist-packages (from vot-toolkit==0.6.4) (0.56.4)\n",
            "Requirement already satisfied: requests>=2.22 in /usr/local/lib/python3.10/dist-packages (from vot-toolkit==0.6.4) (2.27.1)\n",
            "Requirement already satisfied: colorama>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from vot-toolkit==0.6.4) (0.4.6)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.10/dist-packages (from vot-toolkit==0.6.4) (23.1)\n",
            "Requirement already satisfied: dominate>=2.5 in /usr/local/lib/python3.10/dist-packages (from vot-toolkit==0.6.4) (2.8.0)\n",
            "Requirement already satisfied: cachetools>=4.1 in /usr/local/lib/python3.10/dist-packages (from vot-toolkit==0.6.4) (5.3.1)\n",
            "Requirement already satisfied: bidict>=0.19 in /usr/local/lib/python3.10/dist-packages (from vot-toolkit==0.6.4) (0.22.1)\n",
            "Requirement already satisfied: phx-class-registry>=3.0 in /usr/local/lib/python3.10/dist-packages (from vot-toolkit==0.6.4) (4.0.6)\n",
            "Requirement already satisfied: attributee>=0.1.8 in /usr/local/lib/python3.10/dist-packages (from vot-toolkit==0.6.4) (0.1.8)\n",
            "Requirement already satisfied: lazy-object-proxy>=1.9 in /usr/local/lib/python3.10/dist-packages (from vot-toolkit==0.6.4) (1.9.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2->vot-toolkit==0.6.4) (23.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2->vot-toolkit==0.6.4) (0.19.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->vot-toolkit==0.6.4) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->vot-toolkit==0.6.4) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->vot-toolkit==0.6.4) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->vot-toolkit==0.6.4) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->vot-toolkit==0.6.4) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->vot-toolkit==0.6.4) (2.8.2)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.47->vot-toolkit==0.6.4) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.47->vot-toolkit==0.6.4) (67.7.2)\n",
            "Requirement already satisfied: ordered-set in /usr/local/lib/python3.10/dist-packages (from pylatex>=1.3->vot-toolkit==0.6.4) (4.1.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22->vot-toolkit==0.6.4) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22->vot-toolkit==0.6.4) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22->vot-toolkit==0.6.4) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22->vot-toolkit==0.6.4) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Up7CEnJriAVu"
      },
      "outputs": [],
      "source": [
        "# @title Prerequisites\n",
        "import colorsys\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "from typing import Tuple, List, Dict, Any\n",
        "import zipfile\n",
        "\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import imageio\n",
        "import moviepy.editor as mvp\n",
        "import numpy as np\n",
        "import requests\n",
        "import vot\n",
        "from vot.region import calculate_overlaps as calculate_region_overlaps\n",
        "from vot.region import Polygon, Rectangle, Special"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Utility functions\n",
        "def download_and_unzip(url: str, destination: str):\n",
        "  \"\"\"Downloads and unzips a .zip file to a destination.\n",
        "\n",
        "  Downloads a file from the specified URL, saves it to the destination\n",
        "  directory, and then extracts its contents.\n",
        "\n",
        "  If the file is larger than 1GB, it will be downloaded in chunks,\n",
        "  and the download progress will be displayed.\n",
        "\n",
        "  Args:\n",
        "    url (str): The URL of the file to download.\n",
        "    destination (str): The destination directory to save the file and\n",
        "      extract its contents.\n",
        "  \"\"\"\n",
        "  if not os.path.exists(destination):\n",
        "    os.makedirs(destination)\n",
        "\n",
        "  filename = url.split('/')[-1]\n",
        "  file_path = os.path.join(destination, filename)\n",
        "\n",
        "  if os.path.exists(file_path):\n",
        "    print(f'{filename} already exists. Skipping download.')\n",
        "    return\n",
        "\n",
        "  response = requests.get(url, stream=True)\n",
        "  total_size = int(response.headers.get('content-length', 0))\n",
        "  gb = 1024*1024*1024\n",
        "\n",
        "  if total_size / gb > 1:\n",
        "    print(f'{filename} is larger than 1GB, downloading in chunks')\n",
        "    chunk_flag = True\n",
        "    chunk_size = int(total_size/100)\n",
        "  else:\n",
        "    chunk_flag = False\n",
        "    chunk_size = total_size\n",
        "\n",
        "  with open(file_path, 'wb') as file:\n",
        "    for chunk_idx, chunk in enumerate(\n",
        "        response.iter_content(chunk_size=chunk_size)):\n",
        "      if chunk:\n",
        "        if chunk_flag:\n",
        "          print(f\"\"\"{chunk_idx}% downloading\n",
        "          {round((chunk_idx*chunk_size)/gb, 1)}GB\n",
        "          / {round(total_size/gb, 1)}GB\"\"\")\n",
        "        file.write(chunk)\n",
        "  print(f\"'{filename}' downloaded successfully.\")\n",
        "\n",
        "  with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(destination)\n",
        "  print(f\"'{filename}' extracted successfully.\")\n",
        "\n",
        "  os.remove(file_path)\n",
        "\n",
        "\n",
        "def load_db_json(db_file: str) -> Dict[str, Any]:\n",
        "  \"\"\"Loads a JSON file as a dictionary.\n",
        "\n",
        "  Args:\n",
        "    db_file (str): Path to the JSON file.\n",
        "\n",
        "  Returns:\n",
        "    Dict: Loaded JSON data as a dictionary.\n",
        "\n",
        "  Raises:\n",
        "    FileNotFoundError: If the specified file doesn't exist.\n",
        "    TypeError: If the JSON file is not formatted as a dictionary.\n",
        "  \"\"\"\n",
        "  if not os.path.isfile(db_file):\n",
        "    raise FileNotFoundError(f'No such file: {db_file}')\n",
        "\n",
        "  with open(db_file, 'r') as f:\n",
        "    db_file_dict = json.load(f)\n",
        "    if not isinstance(db_file_dict, dict):\n",
        "      raise TypeError('JSON file is not formatted as a dictionary.')\n",
        "    return db_file_dict\n",
        "\n",
        "\n",
        "def load_mp4_to_frames(filename: str) -> np.array:\n",
        "  \"\"\"Loads an MP4 video file and returns its frames as a NumPy array.\n",
        "\n",
        "  Args:\n",
        "    filename (str): Path to the MP4 video file.\n",
        "\n",
        "  Returns:\n",
        "    np.array: Frames of the video as a NumPy array.\n",
        "  \"\"\"\n",
        "  assert os.path.exists(filename), f'File {filename} does not exist.'\n",
        "  cap = cv2.VideoCapture(filename)\n",
        "\n",
        "  num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "  height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "  width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "\n",
        "  vid_frames = np.empty((num_frames, height, width, 3), dtype=np.uint8)\n",
        "\n",
        "  idx = 0\n",
        "  while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "      break\n",
        "\n",
        "    vid_frames[idx] = frame\n",
        "    idx += 1\n",
        "\n",
        "  cap.release()\n",
        "  return vid_frames\n",
        "\n",
        "\n",
        "def get_video_frames(data_item: Dict, video_path: str) -> np.array:\n",
        "  \"\"\"Loads frames of a video specified by an item dictionary.\n",
        "\n",
        "  Assumes format of annotations used in the Perception Test Dataset.\n",
        "\n",
        "  Args:\n",
        "  \tdata_item (Dict): Item from dataset containing metadata.\n",
        "    video_path (str): Path to the directory containing videos.\n",
        "\n",
        "  Returns:\n",
        "    np.array: Frames of the video as a NumPy array.\n",
        "  \"\"\"\n",
        "  video_file_path = os.path.join(video_path,\n",
        "                                 data_item['metadata']['video_id']) + '.mp4'\n",
        "  vid_frames = load_mp4_to_frames(video_file_path)\n",
        "  assert data_item['metadata']['num_frames'] == vid_frames.shape[0]\n",
        "  return vid_frames\n",
        "\n",
        "\n",
        "def get_start_frame(track_arr: List[List[float]]) -> int:\n",
        "  \"\"\"Returns index of the first non-zero element in a track array.\n",
        "\n",
        "  Args:\n",
        "    track_arr (list): one hot vector correspoinding to annotations,\n",
        "      showing which index to start tracking .\n",
        "\n",
        "  Returns:\n",
        "    int: Index of the first non-zero element in the track array.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: Raises error if the length of the array is 0\n",
        "      or if there is no one-hot value.\n",
        "  \"\"\"\n",
        "  if not track_arr or np.count_nonzero(track_arr) == 0:\n",
        "    raise ValueError('Track is empty or has no non-zero elements')\n",
        "  return np.nonzero(track_arr)[0][0]\n",
        "\n",
        "\n",
        "def get_start_info(track: Dict[str, Any]) -> Dict[str, Any]:\n",
        "  \"\"\"Retrieve information about the start frame of a track.\n",
        "\n",
        "  Args:\n",
        "    track (Dict): A dictionary containing information about the track.\n",
        "\n",
        "  Returns:\n",
        "    Dict[str: Any]: A dictionary with the following keys:\n",
        "      'start_id': The frame ID of the start frame.\n",
        "      'start_bounding_box': The bounding box coordinates of the start\n",
        "        frame.\n",
        "      'start_idx': The index of the start frame in the\n",
        "      'bounding_boxes' list.\n",
        "  \"\"\"\n",
        "  track_start_idx = get_start_frame(track['initial_tracking_box'])\n",
        "  track_start_id = track['frame_ids'][track_start_idx]\n",
        "  track_start_bb = track['bounding_boxes'][track_start_idx]\n",
        "\n",
        "  return {'start_id': track_start_id,\n",
        "          'start_bounding_box': track_start_bb,\n",
        "          'start_idx': track_start_idx}"
      ],
      "metadata": {
        "id": "1q_JZ0ueikxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download data\n",
        "data_path = './data/'\n",
        "\n",
        "# This is the Eval.ai challenge subset of object tracking annotations\n",
        "challenge_valid_annot_url = 'https://storage.googleapis.com/dm-perception-test/zip_data/sot_valid_annotations_challenge2023.zip'\n",
        "download_and_unzip(challenge_valid_annot_url, data_path)\n",
        "\n",
        "# validation videos not downloaded because they are too big (approx 70GB).\n",
        "# not needed for this baseline since we are assuming static boxes\n",
        "# we do not actually need the videos to calculate the performance.\n",
        "# valid_videos_url =\n",
        "# 'https://storage.googleapis.com/dm-perception-test/zip_data/sot_valid_videos_challenge2023.zip'\n",
        "# download_and_unzip(valid_videos_url, data_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "on4M6kzSwQ22",
        "outputId": "b7c0c0d7-3afd-468b-9bae-07ea47d6577b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'sample_annotations.zip' downloaded successfully.\n",
            "'sample_annotations.zip' extracted successfully.\n",
            "'sample_videos.zip' downloaded successfully.\n",
            "'sample_videos.zip' extracted successfully.\n",
            "'sot_valid_annotations_challenge2023.zip' downloaded successfully.\n",
            "'sot_valid_annotations_challenge2023.zip' extracted successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Dataset class\n",
        "class PerceptionDataset():\n",
        "  \"\"\"Dataset class to store video items from dataset.\n",
        "\n",
        "  Attributes:\n",
        "    video_folder_path: Path to the folder containing the videos.\n",
        "    task: Task type for annotations.\n",
        "    split: Dataset split to load.\n",
        "  \ttask_db: List containing annotations for dataset according to\n",
        "  \t\tsplit and task availability.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, db_path: Dict[str, Any], video_folder_path: str,\n",
        "               task: str, split: str) -> None:\n",
        "    \"\"\"Initializes the PerceptionDataset class.\n",
        "\n",
        "    Args:\n",
        "      db_path (str): Path to the annotation file.\n",
        "      video_folder_path (str): Path to the folder containing the videos.\n",
        "      task (str): Task type for annotations.\n",
        "      split (str): Dataset split to load.\n",
        "    \"\"\"\n",
        "    self.video_folder_path = video_folder_path\n",
        "    self.task = task\n",
        "    self.split = split\n",
        "    self.task_db = self.load_dataset(db_path)\n",
        "\n",
        "  def load_dataset(self, db_path: str) -> List:\n",
        "    \"\"\"Loads the dataset from the annotation file and processes.\n",
        "\n",
        "    Dict is processed according to split and task.\n",
        "\n",
        "    Args:\n",
        "      db_path (str): Path to the annotation file.\n",
        "\n",
        "    Returns:\n",
        "      List: List of database items containing annotations.\n",
        "    \"\"\"\n",
        "    db_dict = load_db_json(db_path)\n",
        "    db_list = []\n",
        "    for _, val in db_dict.items():\n",
        "      if val['metadata']['split'] == self.split:\n",
        "        if val[self.task]:  # If video has annotations for this task\n",
        "          db_list.append(val)\n",
        "\n",
        "    return db_list\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    \"\"\"Returns the total number of videos in the dataset.\n",
        "\n",
        "    Returns:\n",
        "      int: Total number of videos.\n",
        "    \"\"\"\n",
        "    return len(self.pt_db_list)\n",
        "\n",
        "  def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
        "    \"\"\"Returns the video and annotations for a given index.\n",
        "\n",
        "    Args:\n",
        "      idx (int): Index of the video.\n",
        "\n",
        "    Returns:\n",
        "      Dict: Dictionary containing the video frames, metadata, annotations.\n",
        "    \"\"\"\n",
        "    data_item = self.task_db[idx]\n",
        "    annot = data_item[self.task]\n",
        "\n",
        "    metadata = data_item['metadata']\n",
        "    # here we are loading a placeholder as the frames\n",
        "    # the commented out function below will actually load frames\n",
        "    vid_frames = np.zeros((metadata['num_frames'], 1, 1, 1))\n",
        "    # frames = get_video_frames(video_item, self.video_folder_path)\n",
        "\n",
        "    return {'metadata': metadata,\n",
        "            self.task: annot,\n",
        "            'frames': vid_frames}"
      ],
      "metadata": {
        "id": "cA7ehjrKgeHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Object tracking model (static baseline)\n",
        "class ObjectTracker():\n",
        "  \"\"\"Object tracker class that tracks a given object in a video.\n",
        "\n",
        "  This model assumes static boxes, given the first bounding box\n",
        "  which should be tracked in a sequence, for every frame in the\n",
        "  remaining sequence it will return the same coordinates.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    \"\"\"Initializes the ObjectTracker class.\"\"\"\n",
        "    pass\n",
        "\n",
        "  def track_object_in_video(self, frames: np.array, start_info: Dict[str, Any]\n",
        "                            )-> Dict[str, Any]:\n",
        "    \"\"\"Tracks an object in a video.\n",
        "\n",
        "    Tracks an object given a sequence of frames and initial information about\n",
        "    the coordinates and frame ID of the object.\n",
        "\n",
        "    Args:\n",
        "      frames (np.array): Array of frames representing the video.\n",
        "      start_info (Dict): Dictionary containing the start bounding box and\n",
        "        frame ID.\n",
        "\n",
        "    Returns:\n",
        "      Dict[str: List]: Dictionary containing the tracked bounding boxes and\n",
        "        corresponding frame IDs.\n",
        "    \"\"\"\n",
        "    # initially take starting bounding box for tracking\n",
        "    prev_bb = start_info['start_bounding_box']\n",
        "    output_bounding_boxes = []\n",
        "    output_frame_ids = []\n",
        "\n",
        "    for frame_id in range(start_info['start_id'], frames.shape[0]):\n",
        "      frame = frames[frame_id]\n",
        "      # here is where the per frame tracking is done by the model\n",
        "      # we just return the starting coords in this dummy baseline\n",
        "      bb = self.track_object_in_frame(frame, prev_bb)\n",
        "      output_bounding_boxes.append(bb)\n",
        "      output_frame_ids.append(frame_id)\n",
        "\n",
        "    output_bounding_boxes = np.stack(output_bounding_boxes, axis=0)\n",
        "    output_frame_ids = np.array(output_frame_ids)\n",
        "    return output_bounding_boxes, output_frame_ids\n",
        "\n",
        "  # model inference would be inserted here!!\n",
        "  def track_object_in_frame(self, frame: np.array,\n",
        "                            prev_bb: List[float]) -> List[float]:\n",
        "    \"\"\"Tracks an object in a single frame.\n",
        "\n",
        "    Tracks an object in a single frame based on the previous bounding box\n",
        "    coordinates. Placeholder function that just returns the coords it is given,\n",
        "    assumes a static object.\n",
        "\n",
        "    Args:\n",
        "      frame (np.array): The current frame.\n",
        "      prev_bb(List): Previous bounding box coordinates. (y2,x2,y1,x1)\n",
        "\n",
        "    Returns:\n",
        "      List: The tracked bounding box coordinates in the current frame.\n",
        "    \"\"\"\n",
        "    del frame  # unused\n",
        "    return prev_bb"
      ],
      "metadata": {
        "id": "Z6rOLKF6oUAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Evaluation functions\n",
        "# https://github.com/votchallenge/toolkit/blob/master/vot/analysis/supervised.py\n",
        "def bbox2region(bbox: np.array) -> Rectangle:\n",
        "  \"\"\"Convert bbox to Rectangle or Polygon Class object.\n",
        "\n",
        "  Args:\n",
        "    bbox (ndarray): the format of rectangle bbox is (x1, y1, w, h);\n",
        "      the format of polygon is (x1, y1, x2, y2, ...).\n",
        "\n",
        "  Returns:\n",
        "    Rectangle or Polygon Class object.\n",
        "\n",
        "  Raises:\n",
        "  \tNotImplementedError: Returns error if unexpected number of coordinates in\n",
        "      shape.\n",
        "  \"\"\"\n",
        "\n",
        "  if len(bbox) == 1:\n",
        "    return Special(bbox[0])\n",
        "  elif len(bbox) == 4:\n",
        "    return Rectangle(bbox[0], bbox[1], bbox[2], bbox[3])\n",
        "  elif len(bbox) % 2 == 0 and len(bbox) > 4:\n",
        "    return Polygon([(x_, y_) for x_, y_ in zip(bbox[::2], bbox[1::2])])\n",
        "  else:\n",
        "    raise NotImplementedError(\n",
        "        f'The length of bbox is {len(bbox)}, which is not supported')\n",
        "\n",
        "\n",
        "def trajectory2region(trajectory: List) -> List:\n",
        "  \"\"\"Convert bbox trajectory to Rectangle or Polygon Class object trajectory.\n",
        "\n",
        "  Args:\n",
        "    trajectory (list[ndarray]): The outer list contains bbox of\n",
        "      each frame in a video. The bbox is a ndarray.\n",
        "\n",
        "  Returns:\n",
        "    List: contains the Region Class object of each frame in a\n",
        "      trajectory.\n",
        "  \"\"\"\n",
        "  traj_region = []\n",
        "  for bbox in trajectory:\n",
        "    traj_region.append(bbox2region(bbox))\n",
        "  return traj_region\n",
        "\n",
        "\n",
        "def calc_accuracy(gt_trajectory: List, pred_trajectory: List) -> float:\n",
        "  \"\"\"Calculate accuracy over the sequence.\n",
        "\n",
        "  Args:\n",
        "    gt_trajectory (list[list]): list of bboxes\n",
        "    pred_trajectory (list[ndarray]): The outer list contains the\n",
        "      tracking results of each frame in one video. The ndarray has two cases:\n",
        "      - bbox: denotes the normal tracking box in [x1, y1, w, h]\n",
        "      \tformat.\n",
        "      - special tracking state: [0] denotes the unknown state,\n",
        "      \tnamely the skipping frame after failure, [1] denotes the\n",
        "        initialized state, and [2] denotes the failed state.\n",
        "\n",
        "  Returns:\n",
        "    Float: accuracy over the sequence.\n",
        "  \"\"\"\n",
        "  pred_traj_region = trajectory2region(pred_trajectory)\n",
        "  gt_traj_region = trajectory2region(gt_trajectory)\n",
        "  overlaps = np.array(calculate_region_overlaps(pred_traj_region,\n",
        "                                                gt_traj_region))\n",
        "  mask = np.ones(len(overlaps), dtype=bool)\n",
        "  return np.mean(overlaps[mask]) if any(mask) else 0.\n",
        "\n",
        "\n",
        "def filter_pred_boxes(pred_bb: np.ndarray, pred_fid: np.ndarray,\n",
        "                      gt_fid: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
        "  \"\"\"Filter bounding boxes and frame IDs based on ground truth frame IDs.\n",
        "\n",
        "  Args:\n",
        "    pred_bb (np.ndarray): Array of predicted bounding boxes.\n",
        "    pred_fid (np.ndarray): Array of frame IDs for predicted bounding boxes.\n",
        "    gt_fid (np.ndarray): Array of frame IDs for ground truth bounding boxes.\n",
        "\n",
        "  Returns:\n",
        "    Tuple[np.ndarray, np.ndarray]: Filtered predicted bounding boxes and\n",
        "    \ttheir corresponding frame IDs.\n",
        "  \"\"\"\n",
        "  pred_idx = np.isin(pred_fid, gt_fid).nonzero()[0]\n",
        "  filter_pred_bb = pred_bb[pred_idx]\n",
        "  filter_pred_fid = pred_fid[pred_idx]\n",
        "  return filter_pred_bb, filter_pred_fid\n",
        "\n",
        "\n",
        "def run_iou(boxes: Dict[str, Any], db_dict: Dict[str, Any], filter=True) -> Dict[str, Any]:\n",
        "  \"\"\"Calculate IoU per track and per video.\n",
        "\n",
        "  Calculate Intersection over Union (IoU) for predicted and ground truth\n",
        "    bounding boxes for all tracks in the provided outputs.\n",
        "\n",
        "  Args:\n",
        "    boxes (Dict): Dict containing predicted and label bounding boxes for each\n",
        "      video. Boxes must be in format [x1,y1,x2,y2].\n",
        "    db_dict (Dict): Dict containing annotations.\n",
        "\n",
        "  Returns:\n",
        "    Dict: A dictionary with video IDs as keys and\n",
        "      a list of IoU scores as values.\n",
        "  \"\"\"\n",
        "  all_vot_iou = {}\n",
        "  for vid_id, pred_tracks in boxes.items():\n",
        "    gt_tracks = db_dict[vid_id]['object_tracking']\n",
        "\n",
        "    video_iou = {}\n",
        "    for pred_track in pred_tracks:\n",
        "      gt_track = gt_tracks[pred_track['id']]\n",
        "      # check track IDs\n",
        "      assert pred_track['id'] == gt_track['id']\n",
        "\n",
        "      start_info = get_start_info(gt_track)\n",
        "      start_idx = start_info['start_idx']\n",
        "      # get bounding boxes from frame ID were tracking is supposed to start +1\n",
        "      gt_bb = np.array(gt_track['bounding_boxes'])[start_idx+1:]\n",
        "      gt_fid = gt_track['frame_ids'][start_idx+1:]\n",
        "      # weird case where only one box is labelled\n",
        "      if not gt_fid:\n",
        "        continue\n",
        "\n",
        "      pred_bb = np.array(pred_track['bounding_boxes'])\n",
        "      pred_fid = np.array(pred_track['frame_ids'])\n",
        "      # filter predicted trajectory for frame IDs where we have annotations\n",
        "      if filter:\n",
        "        pred_bb, pred_fid = filter_pred_boxes(pred_bb, pred_fid, gt_fid)\n",
        "\n",
        "      # check for missing frame IDs in prediction\n",
        "      missing_idx = np.where(np.isin(gt_fid, pred_fid, invert=True))[0]\n",
        "      if missing_idx.size != 0:\n",
        "        raise ValueError(f'Missing IDs from object trajectory: {missing_idx}')\n",
        "      if len(gt_bb) != len(pred_bb):\n",
        "        raise ValueError('Missing boxes in predictions')\n",
        "\n",
        "      #  convert y2,x2,y1,x1 [0,1] to x1,y1,w,h in pixel space\n",
        "      [height, width] = db_dict[vid_id]['metadata']['resolution']\n",
        "      pred_w = pred_bb[:, 2] - pred_bb[:, 0]\n",
        "      pred_h = pred_bb[:, 3] - pred_bb[:, 1]\n",
        "      pred_bb = np.stack([pred_bb[:, 0]*width, pred_bb[:, 1]*height,\n",
        "                          pred_w*width, pred_h*height], axis=1)\n",
        "      gt_w = gt_bb[:, 2] - gt_bb[:, 0]\n",
        "      gt_h = gt_bb[:, 3] - gt_bb[:, 1]\n",
        "      gt_bb = np.stack([gt_bb[:, 0]*width, gt_bb[:, 1]*height,\n",
        "                        gt_w*width, gt_h*height], axis=1)\n",
        "\n",
        "      # compute IoU per track\n",
        "      iou = calc_accuracy(gt_bb, pred_bb)\n",
        "      video_iou[pred_track['id']] = iou\n",
        "\n",
        "    all_vot_iou[vid_id] = video_iou\n",
        "\n",
        "  return all_vot_iou\n",
        "\n",
        "\n",
        "def summarise_results(labels: Dict[str, Any], results: Dict[str, Any]):\n",
        "  \"\"\"Summarise the results according to camera movement.\n",
        "\n",
        "  Summarise the results of a dataset by calculating average IoU scores\n",
        "  across all videos, videos with a static camera and videos with a moving\n",
        "  camera.\n",
        "\n",
        "  Args:\n",
        "    labels (Dict): A dictionary containing metadata and\n",
        "      information about the dataset.\n",
        "    results (Dict): A dictionary containing IoU scores\n",
        "      for each video in the dataset.\n",
        "  \"\"\"\n",
        "  all_ious = []\n",
        "  # aggregate performance based on camera motion for analysis\n",
        "  static_ious = []\n",
        "  moving_ious = []\n",
        "\n",
        "  for vid, iou_dict in results.items():\n",
        "    ious = list(iou_dict.values())\n",
        "    if not ious:\n",
        "      continue\n",
        "\n",
        "    all_ious.append(np.mean(ious))\n",
        "\n",
        "    if labels[vid]['metadata']['is_camera_moving']:\n",
        "      moving_ious.append(np.mean(ious))\n",
        "    else:\n",
        "      static_ious.append(np.mean(ious))\n",
        "\n",
        "  if all_ious:\n",
        "    print(f\"\"\"Average IoU across all videos in dataset:\n",
        "          {np.array(all_ious).mean():.3f}\"\"\")\n",
        "\n",
        "  if static_ious:\n",
        "    print(f\"\"\"Average IoU across static camera videos in dataset:\n",
        "          {np.array(static_ious).mean():.3f}\"\"\")\n",
        "\n",
        "  if moving_ious:\n",
        "    print(f\"\"\"Average IoU across moving camera videos in dataset:\n",
        "          {np.array(moving_ious).mean():.3f}\"\"\")"
      ],
      "metadata": {
        "id": "NdTACVGWrSOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Evaluate static baseline\n",
        "label_path = './data/object_tracking_valid_subset.json'\n",
        "cfg = {'video_folder_path': './data/videos/',\n",
        "       'task': 'object_tracking',\n",
        "       'split': 'valid'}\n",
        "\n",
        "# when true the bounding boxes and frame IDs will be filtered to demonstrate\n",
        "# how to submit for the subset of annotated frames which will be used for\n",
        "# evaluation\n",
        "filter_outputs = True\n",
        "\n",
        "# init dataset\n",
        "tracking_dataset = PerceptionDataset(label_path, **cfg)\n",
        "\n",
        "# init tracking model\n",
        "object_tracker = ObjectTracker()\n",
        "\n",
        "# run model across full dataset\n",
        "results = {}\n",
        "for video_item in tracking_dataset:\n",
        "  video_id = video_item['metadata']['video_id']\n",
        "  video_pred_tracks = []\n",
        "  for gt_track in video_item['object_tracking']:\n",
        "    start_info = get_start_info(gt_track)\n",
        "    pred_bounding_boxes, pred_frame_ids = (\n",
        "        object_tracker.track_object_in_video(video_item['frames'], start_info)\n",
        "    )\n",
        "\n",
        "    # filtering bounding boxes and frame IDs\n",
        "    if filter_outputs:\n",
        "      pred_bounding_boxes, pred_frame_ids = (\n",
        "          filter_pred_boxes(pred_bounding_boxes, pred_frame_ids,\n",
        "                            gt_track['frame_ids'])\n",
        "      )\n",
        "\n",
        "    pred_track = {}\n",
        "    # .tolist() to serialise without error\n",
        "    pred_track['bounding_boxes'] = pred_bounding_boxes.tolist()\n",
        "    pred_track['frame_ids'] = pred_frame_ids.tolist()\n",
        "    pred_track['id'] = gt_track['id']\n",
        "    video_pred_tracks.append(pred_track)\n",
        "\n",
        "  results[video_id] = video_pred_tracks"
      ],
      "metadata": {
        "id": "0Yn588DzlhUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Compute average IoU\n",
        "label_dict = load_db_json(label_path)\n",
        "iou_results = run_iou(results, label_dict, filter=filter_outputs)\n",
        "summarise_results(label_dict, iou_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enFk726JX_-c",
        "outputId": "b93ae411-e8c2-45c2-fcca-f41d102e0d12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average IoU across all videos in dataset:\n",
            "          0.639\n",
            "Average IoU across static camera videos in dataset:\n",
            "          0.676\n",
            "Average IoU across moving camera videos in dataset:\n",
            "          0.397\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Serialise example results file\n",
        "# Writing model outputs in the expected competition format. This\n",
        "# JSON file contains answers to all questions in the validation split in the\n",
        "# format:\n",
        "\n",
        "# example_submission = {'video_1009' :{'object_tracking':[\n",
        "#     {'id': 0, 'bounding_boxes': [[x1,x2,y1,y2],...], 'frame_ids': [n,...]},\n",
        "#     {'id': 1, 'bounding_boxes': [[x1,x2,y1,y2],...], 'frame_ids': [n,...]}]}}\n",
        "\n",
        "# This file could be used directly as a submission for the Eval.ai challenge\n",
        "with open(f'{cfg[\"task\"]}_{cfg[\"split\"]}_results.json', 'w') as my_file:\n",
        "  json.dump(results, my_file)\n",
        "\n",
        "del results\n",
        "del iou_results"
      ],
      "metadata": {
        "id": "h_dEcM__pV90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Visualisation functions\n",
        "def get_colors(num_colors: int) -> Tuple[int, int, int]:\n",
        "  \"\"\"Generate random colormaps for visualizing different objects.\n",
        "\n",
        "  Args:\n",
        "    num_colors (int): The number of colors to generate.\n",
        "\n",
        "  Returns:\n",
        "    Tuple[int, int, int]: A tuple of RGB values representing the\n",
        "      generated colors.\n",
        "  \"\"\"\n",
        "  colors = []\n",
        "  for i in np.arange(0., 360., 360. / num_colors):\n",
        "    hue = i / 360.\n",
        "    lightness = (50 + np.random.rand() * 10) / 100.\n",
        "    saturation = (90 + np.random.rand() * 10) / 100.\n",
        "    color = colorsys.hls_to_rgb(hue, lightness, saturation)\n",
        "    color = (int(color[0] * 255), int(color[1] * 255), int(color[2] * 255))\n",
        "    colors.append(color)\n",
        "  random.seed(0)\n",
        "  random.shuffle(colors)\n",
        "  return colors\n",
        "\n",
        "\n",
        "def display_video(vid_frames: np.array, fps: int = 30):\n",
        "  \"\"\"Create and display temporary video from numpy array frames.\n",
        "\n",
        "  Args:\n",
        "    vid_frames: (np.array): The frames of the video as a\n",
        "    \tnumpy array. Format of frames should be:\n",
        "    \t(num_frames, height, width, channels)\n",
        "    fps (int): Frames per second for the video playback. Default is 30.\n",
        "  \"\"\"\n",
        "  kwargs = {'macro_block_size': None}\n",
        "  imageio.mimwrite('tmp_video_display.mp4',\n",
        "                   vid_frames[:, :, :, ::-1], fps=fps, **kwargs)\n",
        "  display(mvp.ipython_display('tmp_video_display.mp4'))\n",
        "\n",
        "\n",
        "def display_frame(frame: np.array):\n",
        "  \"\"\"Display a frame, converting from RGB to BGR for cv2.\n",
        "\n",
        "  Args:\n",
        "    frame (np.array): The frame to be displayed.\n",
        "  \"\"\"\n",
        "  cv2_imshow(frame)\n",
        "\n",
        "\n",
        "def paint_box(video: np.array, track: Dict[str, Any],\n",
        "              color: Tuple[int, int, int] = (255, 0, 0),\n",
        "              addn_label: str = '') -> np.array:\n",
        "  \"\"\"Paint bounding box and label on video for a given track.\n",
        "\n",
        "  Args:\n",
        "    video (np.array): The video frames as a numpy array.\n",
        "    track (Dict): The track information containing bounding box\n",
        "    and frame information, assumes Perception Test Dataset format.\n",
        "    color (Tuple[int, int, int]): The RGB color values for the bounding box.\n",
        "      Default is red (255, 0, 0).\n",
        "    addn_label (str): Additional label to be added to the track label.\n",
        "      Default is an empty string.\n",
        "\n",
        "  Returns:\n",
        "    np.array: The modified video frames with painted bounding box and\n",
        "      label.\n",
        "  \"\"\"\n",
        "  _, height, width, _ = video.shape\n",
        "  name = str(track['id']) + ' : ' + track['label'] + addn_label\n",
        "  bounding_boxes = np.array(track['bounding_boxes'])\n",
        "\n",
        "  for box, frame_id in zip(bounding_boxes, track['frame_ids']):\n",
        "    frame = np.array(video[frame_id])\n",
        "    x1 = int(round(box[0] * width))\n",
        "    y1 = int(round(box[1] * height))\n",
        "    x2 = int(round(box[2] * width))\n",
        "    y2 = int(round(box[3] * height))\n",
        "    frame = cv2.rectangle(frame, (x1, y1), (x2, y2),\n",
        "                          color=color, thickness=2)\n",
        "    frame = cv2.putText(frame, name, (x1, y1 + 20),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.75, color, 2)\n",
        "    video[frame_id] = frame\n",
        "\n",
        "  return video\n",
        "\n",
        "\n",
        "def paint_boxes(video: np.array, tracks: List[Dict],\n",
        "                colors: Tuple[int, int, int]) -> np.array:\n",
        "  \"\"\"Paint bounding boxes and labels on a video for multiple tracks.\n",
        "\n",
        "  Args:\n",
        "    video (np.array): The video frames as a numpy array.\n",
        "    tracks (List): A list of track information,\n",
        "      where each track contains bounding box and frame information.\n",
        "    colors (Tuple): Tuple containing randomly generated RGB color values.\n",
        "\n",
        "  Returns:\n",
        "    np.array: The modified video frames with painted bounding boxes\n",
        "      and labels.\n",
        "  \"\"\"\n",
        "  for i, track in enumerate(tracks):\n",
        "    video = paint_box(video, track, colors[i])\n",
        "  return video"
      ],
      "metadata": {
        "id": "dafGHrzlPnBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Ground truth annotations visualised\n",
        "COLORS = get_colors(num_colors=100)\n",
        "\n",
        "# sample annotations and videos to visualise the annotations later\n",
        "sample_annot_url = 'https://storage.googleapis.com/dm-perception-test/zip_data/sample_annotations.zip'\n",
        "download_and_unzip(sample_annot_url, data_path)\n",
        "\n",
        "sample_videos_url = 'https://storage.googleapis.com/dm-perception-test/zip_data/sample_videos.zip'\n",
        "download_and_unzip(sample_videos_url, data_path)\n",
        "\n",
        "# get sample video info to showcase the annotations\n",
        "sample_db_path = './data/sample.json'\n",
        "sample_db_dict = load_db_json(sample_db_path)\n",
        "video_id = list(sample_db_dict.keys())[0]\n",
        "video_item = sample_db_dict[video_id]\n",
        "frames = get_video_frames(video_item, cfg['video_folder_path'])\n",
        "\n",
        "tracks_to_show = 0.5  # @param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "num_tracks = int(len(video_item['object_tracking']) * tracks_to_show)\n",
        "frames = paint_boxes(frames, video_item['object_tracking'][0 : num_tracks],\n",
        "                     COLORS)\n",
        "\n",
        "annotated_frames = []\n",
        "for frame_idx in video_item['object_tracking'][0]['frame_ids']:\n",
        "  annotated_frames.append(frames[frame_idx])\n",
        "\n",
        "annotated_frames = np.array(annotated_frames)\n",
        "display_video(annotated_frames, 1)\n",
        "del frames\n",
        "del annotated_frames"
      ],
      "metadata": {
        "id": "hPJkHKWPEvff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Model outputs visualised (static boxes)\n",
        "# here we show how actual inference would work with video frames loaded\n",
        "frames = get_video_frames(video_item, cfg['video_folder_path'])\n",
        "\n",
        "pred_tracks = []\n",
        "for gt_track in video_item['object_tracking']:\n",
        "  start_info = get_start_info(gt_track)\n",
        "  pred_bounding_boxes, pred_frame_ids = (\n",
        "      object_tracker.track_object_in_video(frames, start_info)\n",
        "  )\n",
        "  pred_track = {}\n",
        "  pred_track['bounding_boxes'] = pred_bounding_boxes.tolist()\n",
        "  pred_track['frame_ids'] = pred_frame_ids\n",
        "  pred_track['label'] = gt_track['label']\n",
        "  pred_track['id'] = gt_track['id']\n",
        "  pred_tracks.append(pred_track)\n",
        "\n",
        "tracks_to_show = 0.5  # @param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "num_tracks = int(len(pred_tracks) * tracks_to_show)\n",
        "frames = paint_boxes(frames, pred_tracks[0: num_tracks], COLORS)\n",
        "\n",
        "# can display video at full fps since all frames have bounding boxes from\n",
        "# the model output\n",
        "display_video(frames, video_item['metadata']['frame_rate'])\n",
        "del frames"
      ],
      "metadata": {
        "id": "oVFlyvJFNW-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Comparing ground truth labels vs model outputs\n",
        "track_to_compare = 1  # @param {type:\"integer\"}\n",
        "\n",
        "if track_to_compare > len(video_item['object_tracking']):\n",
        "  raise ValueError(f'Track {track_to_compare} is not in the video')\n",
        "\n",
        "frames = get_video_frames(video_item, cfg['video_folder_path'])\n",
        "frames = paint_box(frames, video_item['object_tracking'][track_to_compare],\n",
        "                   color=COLORS[0], addn_label=': label')\n",
        "frames = paint_box(frames, pred_tracks[track_to_compare], color=COLORS[1],\n",
        "                   addn_label=': predicted')\n",
        "\n",
        "annotated_frames = []\n",
        "for frame_idx in video_item['object_tracking'][track_to_compare]['frame_ids']:\n",
        "  annotated_frames.append(frames[frame_idx])\n",
        "\n",
        "annotated_frames = np.array(annotated_frames)\n",
        "display_video(annotated_frames, 1)\n",
        "del frames"
      ],
      "metadata": {
        "id": "cKC8IGOjWbRf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}

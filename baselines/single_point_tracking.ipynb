{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepmind/perception_test/blob/main/baselines/single_point_tracking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook demonstrates how to load the point tracking annotations in the validation split of the Perception Test and run the evaluation for a simple baseline model which predicts static points for all tracks."
      ],
      "metadata": {
        "id": "JykaDwpmWOcu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copyright 2023 DeepMind Technologies Limited\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at [https://www.apache.org/licenses/LICENSE-2.0](https://www.apache.org/licenses/LICENSE-2.0).\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n"
      ],
      "metadata": {
        "id": "Ov2zD0apsX3A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Single Point Tracking Static Baseline\n",
        "\n",
        "Github: https://github.com/deepmind/perception_test\n",
        "\n",
        "## The Perception Test\n",
        "[Perception Test: A Diagnostic Benchmark for Multimodal Video Models](https://arxiv.org/abs/2305.13786) is a multimodal benchmark designed to comprehensively evaluate the perception and reasoning skills of multimodal video models. The Perception Test dataset introduces real-world videos designed to show perceptually interesting situations and defines multiple computational tasks (object and point tracking, action and sound localisation, multiple-choice and grounded video question-answering). Here, we provide details and a simple baseline for the single  point traking task.\n",
        "\n",
        "[![Perception Test Overview Presentation](https://img.youtube.com/vi/8BiajMOBWdk/maxresdefault.jpg)](https://youtu.be/8BiajMOBWdk?t=10)"
      ],
      "metadata": {
        "id": "NvbIw-q_sYm3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Single Point Tracking\n",
        " In this task, given the 2D coordinates of a point at the beginning of a video, the model should track the point throughout the video. Performance is evaluated using the recently proposed average [Jaccard metric](https://arxiv.org/abs/2211.03726) for evaluating both long-term point tracking position and occlusion accuracy.\n",
        "\n",
        "The below image shows examples of point tracking annotations. Note that the annotations match the frame rate of the original videos, around 30fps.\n",
        "\n",
        "![image collage](https://storage.googleapis.com/dm-perception-test/img/ptpt_collage.png)"
      ],
      "metadata": {
        "id": "SitHmNRNyg43"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Static baseline\n",
        "This notebook demonstrates how to load the point tracking annotations in the validation split of the Perception Test, and run the evaluation for a dummy baseline model. This model assumes static points for all point tracks in a video."
      ],
      "metadata": {
        "id": "WE71zLoQylku"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Up7CEnJriAVu"
      },
      "outputs": [],
      "source": [
        "# @title Prerequisites\n",
        "import colorsys\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "from typing import Tuple, List, Dict, Any, Mapping\n",
        "import zipfile\n",
        "\n",
        "import cv2\n",
        "import imageio\n",
        "import moviepy.editor as mvp\n",
        "import numpy as np\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Utility functions\n",
        "def download_and_unzip(url: str, destination: str):\n",
        "  \"\"\"Downloads and unzips a .zip file to a destination.\n",
        "\n",
        "  Downloads a file from the specified URL, saves it to the destination\n",
        "  directory, and then extracts its contents.\n",
        "\n",
        "  If the file is larger than 1GB, it will be downloaded in chunks,\n",
        "  and the download progress will be displayed.\n",
        "\n",
        "  Args:\n",
        "    url (str): The URL of the file to download.\n",
        "    destination (str): The destination directory to save the file and\n",
        "      extract its contents.\n",
        "  \"\"\"\n",
        "  if not os.path.exists(destination):\n",
        "    os.makedirs(destination)\n",
        "\n",
        "  filename = url.split('/')[-1]\n",
        "  file_path = os.path.join(destination, filename)\n",
        "\n",
        "  if os.path.exists(file_path):\n",
        "    print(f'{filename} already exists. Skipping download.')\n",
        "    return\n",
        "\n",
        "  response = requests.get(url, stream=True)\n",
        "  total_size = int(response.headers.get('content-length', 0))\n",
        "  gb = 1024*1024*1024\n",
        "\n",
        "  if total_size / gb > 1:\n",
        "    print(f'{filename} is larger than 1GB, downloading in chunks')\n",
        "    chunk_flag = True\n",
        "    chunk_size = int(total_size/100)\n",
        "  else:\n",
        "    chunk_flag = False\n",
        "    chunk_size = total_size\n",
        "\n",
        "  with open(file_path, 'wb') as file:\n",
        "    for chunk_idx, chunk in enumerate(\n",
        "        response.iter_content(chunk_size=chunk_size)):\n",
        "      if chunk:\n",
        "        if chunk_flag:\n",
        "          print(f\"\"\"{chunk_idx}% downloading\n",
        "          {round((chunk_idx*chunk_size)/gb, 1)}GB\n",
        "          / {round(total_size/gb, 1)}GB\"\"\")\n",
        "        file.write(chunk)\n",
        "  print(f\"'{filename}' downloaded successfully.\")\n",
        "\n",
        "  with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(destination)\n",
        "  print(f\"'{filename}' extracted successfully.\")\n",
        "\n",
        "  os.remove(file_path)\n",
        "\n",
        "\n",
        "def load_db_json(db_file: str) -> Dict[str, Any]:\n",
        "  \"\"\"Loads a JSON file as a dictionary.\n",
        "\n",
        "  Args:\n",
        "    db_file (str): Path to the JSON file.\n",
        "\n",
        "  Returns:\n",
        "    Dict: Loaded JSON data as a dictionary.\n",
        "\n",
        "  Raises:\n",
        "    FileNotFoundError: If the specified file doesn't exist.\n",
        "    TypeError: If the JSON file is not formatted as a dictionary.\n",
        "  \"\"\"\n",
        "  if not os.path.isfile(db_file):\n",
        "    raise FileNotFoundError(f'No such file: {db_file}')\n",
        "\n",
        "  with open(db_file, 'r') as f:\n",
        "    db_file_dict = json.load(f)\n",
        "    if not isinstance(db_file_dict, dict):\n",
        "      raise TypeError('JSON file is not formatted as a dictionary.')\n",
        "    return db_file_dict\n",
        "\n",
        "\n",
        "def load_mp4_to_frames(filename: str) -> np.array:\n",
        "  \"\"\"Loads an MP4 video file and returns its frames as a NumPy array.\n",
        "\n",
        "  Args:\n",
        "    filename (str): Path to the MP4 video file.\n",
        "\n",
        "  Returns:\n",
        "    np.array: Frames of the video as a NumPy array.\n",
        "  \"\"\"\n",
        "  assert os.path.exists(filename), f'File {filename} does not exist.'\n",
        "  cap = cv2.VideoCapture(filename)\n",
        "\n",
        "  vid_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "  height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "  width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "\n",
        "  vid_frames = np.empty((vid_frames, height, width, 3), dtype=np.uint8)\n",
        "\n",
        "  idx = 0\n",
        "  while True:\n",
        "    ret, vid_frame = cap.read()\n",
        "    if not ret:\n",
        "      break\n",
        "\n",
        "    vid_frames[idx] = vid_frame\n",
        "    idx += 1\n",
        "\n",
        "  cap.release()\n",
        "  return vid_frames\n",
        "\n",
        "\n",
        "def get_video_frames(data_item: Dict[str, Any], vid_path: str) -> np.array:\n",
        "  \"\"\"Loads frames of a video specified by an item dictionary.\n",
        "\n",
        "  Assumes format of annotations used in the Perception Test Dataset.\n",
        "\n",
        "  Args:\n",
        "  \tdata_item (Dict): Item from dataset containing metadata.\n",
        "    vid_path (str): Path to the directory containing videos.\n",
        "\n",
        "  Returns:\n",
        "    np.array: Frames of the video as a NumPy array.\n",
        "  \"\"\"\n",
        "  video_file_path = os.path.join(vid_path,\n",
        "                                 data_item['metadata']['video_id']) + '.mp4'\n",
        "  vid_frames = load_mp4_to_frames(video_file_path)\n",
        "  assert data_item['metadata']['num_frames'] == vid_frames.shape[0]\n",
        "  return vid_frames"
      ],
      "metadata": {
        "id": "1q_JZ0ueikxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download data\n",
        "data_path = './data/'\n",
        "db_json_path = './data/sample.json'\n",
        "video_path = './data/videos/'\n",
        "\n",
        "# sample annotations and videos to visualise the annotations later\n",
        "sample_annot_url = 'https://storage.googleapis.com/dm-perception-test/zip_data/sample_annotations.zip'\n",
        "download_and_unzip(sample_annot_url, data_path)\n",
        "\n",
        "sample_videos_url = 'https://storage.googleapis.com/dm-perception-test/zip_data/sample_videos.zip'\n",
        "download_and_unzip(sample_videos_url, data_path)\n",
        "\n",
        "# This is the validation set of annotations for point tracking\n",
        "valid_annot_url = 'https://storage.googleapis.com/dm-perception-test/zip_data/point_tracking_valid_annotations.zip'\n",
        "download_and_unzip(valid_annot_url, data_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "on4M6kzSwQ22",
        "outputId": "d4d24e8c-2c8a-4b2f-a99d-ca161ef4fafb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'sample_annotations.zip' downloaded successfully.\n",
            "'sample_annotations.zip' extracted successfully.\n",
            "'sample_videos.zip' downloaded successfully.\n",
            "'sample_videos.zip' extracted successfully.\n",
            "'point_tracking_valid_annotations.zip' downloaded successfully.\n",
            "'point_tracking_valid_annotations.zip' extracted successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Dataset class\n",
        "class PerceptionDataset():\n",
        "  \"\"\"Dataset class to store video items from dataset.\n",
        "\n",
        "  Attributes:\n",
        "    video_folder_path: Path to the folder containing the videos.\n",
        "    task: Task type for annotations.\n",
        "    split: Dataset split to load.\n",
        "  \ttask_db: List containing annotations for dataset according to\n",
        "  \t\tsplit and task availability.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, db_path: Dict[str, Any], video_folder_path: str,\n",
        "               task: str, split: str) -> None:\n",
        "    \"\"\"Initializes the PerceptionDataset class.\n",
        "\n",
        "    Args:\n",
        "      db_path (str): Path to the annotation file.\n",
        "      video_folder_path (str): Path to the folder containing the videos.\n",
        "      task (str): Task type for annotations.\n",
        "      split (str): Dataset split to load.\n",
        "    \"\"\"\n",
        "    self.video_folder_path = video_folder_path\n",
        "    self.task = task\n",
        "    self.split = split\n",
        "    self.task_db = self.load_dataset(db_path)\n",
        "\n",
        "  def load_dataset(self, db_path: str) -> List:\n",
        "    \"\"\"Loads the dataset from the annotation file to dictionary.\n",
        "\n",
        "    Dict is processed according to split and task.\n",
        "\n",
        "    Args:\n",
        "      db_path (str): Path to the annotation file.\n",
        "\n",
        "    Returns:\n",
        "      List: List of database items containing annotations.\n",
        "    \"\"\"\n",
        "    db_dict = load_db_json(db_path)\n",
        "    db_list = []\n",
        "    for _, val in db_dict.items():\n",
        "      if val['metadata']['split'] == self.split:\n",
        "        if val[self.task]:  # If video has annotations for this task\n",
        "          db_list.append(val)\n",
        "\n",
        "    return db_list\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    \"\"\"Returns the total number of videos in the dataset.\n",
        "\n",
        "    Returns:\n",
        "      int: Total number of videos.\n",
        "    \"\"\"\n",
        "    return len(self.pt_db_list)\n",
        "\n",
        "  def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
        "    \"\"\"Returns the video and annotations for a given index.\n",
        "\n",
        "    Args:\n",
        "      idx (int): Index of the video.\n",
        "\n",
        "    Returns:\n",
        "      Dict: Dictionary containing the video frames, metadata, annotations.\n",
        "    \"\"\"\n",
        "    data_item = self.task_db[idx]\n",
        "    annot = data_item[self.task]\n",
        "\n",
        "    metadata = data_item['metadata']\n",
        "    # here we are loading a placeholder as the frames\n",
        "    # the commented out function below will actually load frames\n",
        "    vid_frames = np.zeros((metadata['num_frames'], 1, 1, 1))\n",
        "    # frames = get_video_frames(video_item, self.video_folder_path)\n",
        "\n",
        "    return {'metadata': metadata,\n",
        "            self.task: annot,\n",
        "            'frames': vid_frames}"
      ],
      "metadata": {
        "id": "cA7ehjrKgeHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Point tracking model (static baseline)\n",
        "class PointTracker():\n",
        "  \"\"\"Point tracker class that tracks a given point in a video.\n",
        "\n",
        "  This model assumes static point: given the starting point in a video\n",
        "  which should be tracked in a sequence, for every frame in the\n",
        "  remaining sequence it will return the same coordinates.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    \"\"\"Initializes the PointTracker class.\"\"\"\n",
        "    pass\n",
        "\n",
        "  def track(self, frames: np.array, start_point: List[float],\n",
        "            start_frame_id: int)-> Dict[str, Any]:\n",
        "    \"\"\"Tracks a point in a video.\n",
        "\n",
        "    Tracks a point given a sequence of frames and initial information about\n",
        "    the coordinates and frame ID of the point.\n",
        "\n",
        "    Args:\n",
        "      frames (np.array): Array of frames representing the video.\n",
        "      start_point (List): List containing the start point in [y, x] format.\n",
        "      start_frame_id (int): Integer value for starting frame ID for point track.\n",
        "\n",
        "    Returns:\n",
        "      Dict: Dictionary containing the point track sequence and\n",
        "        corresponding frame IDs.\n",
        "    \"\"\"\n",
        "    # initially take starting point for tracking\n",
        "    prev_point = start_point\n",
        "    output_points = []\n",
        "    output_frame_ids = []\n",
        "\n",
        "    for frame_id in range(start_frame_id, frames.shape[0]):\n",
        "      # here is where the per frame tracking is done by the model\n",
        "      # we just return the starting coords in this dummy baseline\n",
        "      frame = frames[frame_id]\n",
        "      point = self.track_frame(frame, prev_point)\n",
        "      output_points.append(point)\n",
        "      output_frame_ids.append(frame_id)\n",
        "    output_points = np.stack(output_points, axis=0)\n",
        "    output_frame_ids = np.array(output_frame_ids)\n",
        "    return output_points, output_frame_ids\n",
        "\n",
        "  # model inference would be inserted here!!\n",
        "  def track_frame(self, frame: np.array,\n",
        "                  prev_point: List[float]) -> List[float]:\n",
        "    \"\"\"Tracks a point in a single frame.\n",
        "\n",
        "    Tracks a point in a single frame based on the previous point\n",
        "    coordinates. Placeholder function that just returns the coords it is given,\n",
        "    assumes a static point which is never occluded.\n",
        "\n",
        "    Args:\n",
        "      frame (np.array): The current frame.\n",
        "      prev_point(List): Previous point coordinates. [y, x] format.\n",
        "\n",
        "    Returns:\n",
        "      List: The point coordinates in the current frame.\n",
        "    \"\"\"\n",
        "    del frame  # unused\n",
        "    return prev_point"
      ],
      "metadata": {
        "id": "Z6rOLKF6oUAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Evaluation functions\n",
        "\n",
        "# from https://github.com/deepmind/tapnet/blob/main/evaluation_datasets.py\n",
        "def compute_tapvid_metrics(\n",
        "    query_points: np.ndarray,\n",
        "    gt_occluded: np.ndarray,\n",
        "    gt_tracks: np.ndarray,\n",
        "    pred_occluded: np.ndarray,\n",
        "    pred_tracks: np.ndarray,\n",
        "    query_mode: str,\n",
        ") -> Mapping[str, np.ndarray]:\n",
        "  \"\"\"Computes TAP-Vid metrics (Jaccard, Pts. Within Thresh, Occ. Acc.)\n",
        "\n",
        "  See the TAP-Vid paper for details on the metric computation.  All inputs are\n",
        "  given in raster coordinates.  The first three arguments should be the direct\n",
        "  outputs of the reader: the 'query_points', 'occluded', and 'target_points'.\n",
        "  The paper metrics assume these are scaled relative to 256x256 images.\n",
        "  pred_occluded and pred_tracks are your algorithm's predictions.\n",
        "\n",
        "  This function takes a batch of inputs, and computes metrics separately for\n",
        "  each video.  The metrics for the full benchmark are a simple mean of the\n",
        "  metrics across the full set of videos.  These numbers are between 0 and 1,\n",
        "  but the paper multiplies them by 100 to ease reading.\n",
        "\n",
        "  Args:\n",
        "     query_points: The query points, an in the format [t, y, x].  Its size is\n",
        "       [b, n, 3], where b is the batch size and n is the number of queries\n",
        "     gt_occluded: A boolean array of shape [b, n, t], where t is the number of\n",
        "       frames.  True indicates that the point is occluded.\n",
        "     gt_tracks: The target points, of shape [b, n, t, 2].  Each point is in the\n",
        "       format [x, y]\n",
        "     pred_occluded: A boolean array of predicted occlusions, in the same format\n",
        "       as gt_occluded.\n",
        "     pred_tracks: An array of track predictions from your algorithm, in the same\n",
        "       format as gt_tracks.\n",
        "     query_mode: Either 'first' or 'strided', depending on how queries are\n",
        "       sampled.  If 'first', we assume the prior knowledge that all points\n",
        "       before the query point are occluded, and these are removed from the\n",
        "       evaluation.\n",
        "\n",
        "  Returns:\n",
        "      A dict with the following keys:\n",
        "\n",
        "      occlusion_accuracy: Accuracy at predicting occlusion.\n",
        "      pts_within_{x} for x in [1, 2, 4, 8, 16]: Fraction of points\n",
        "        predicted to be within the given pixel threshold, ignoring occlusion\n",
        "        prediction.\n",
        "      jaccard_{x} for x in [1, 2, 4, 8, 16]: Jaccard metric for the given\n",
        "        threshold\n",
        "      average_pts_within_thresh: average across pts_within_{x}\n",
        "      average_jaccard: average across jaccard_{x}\n",
        "  \"\"\"\n",
        "\n",
        "  metrics = {}\n",
        "\n",
        "  # Don't evaluate the query point.  Numpy doesn't have one_hot, so we\n",
        "  # replicate it by indexing into an identity matrix.\n",
        "  one_hot_eye = np.eye(gt_tracks.shape[2])\n",
        "  query_frame = query_points[..., 0]\n",
        "  query_frame = np.round(query_frame).astype(np.int32)\n",
        "  evaluation_points = one_hot_eye[query_frame] == 0\n",
        "\n",
        "  # If we're using the first point on the track as a query, don't evaluate the\n",
        "  # other points.\n",
        "  if query_mode == 'first':\n",
        "    for i in range(gt_occluded.shape[0]):\n",
        "      index = np.where(gt_occluded[i] == 0)[0][0]\n",
        "      evaluation_points[i, :index] = False\n",
        "  elif query_mode != 'strided':\n",
        "    raise ValueError('Unknown query mode ' + query_mode)\n",
        "\n",
        "  # Occlusion accuracy is simply how often the predicted occlusion equals the\n",
        "  # ground truth.\n",
        "  occ_acc = np.sum(\n",
        "      np.equal(pred_occluded, gt_occluded) & evaluation_points,\n",
        "      axis=(1, 2),\n",
        "  ) / np.sum(evaluation_points)\n",
        "  metrics['occlusion_accuracy'] = occ_acc\n",
        "\n",
        "  # Next, convert the predictions and ground truth positions into pixel\n",
        "  # coordinates.\n",
        "  visible = np.logical_not(gt_occluded)\n",
        "  pred_visible = np.logical_not(pred_occluded)\n",
        "  all_frac_within = []\n",
        "  all_jaccard = []\n",
        "  for thresh in [1, 2, 4, 8, 16]:\n",
        "    # True positives are points that are within the threshold and where both\n",
        "    # the prediction and the ground truth are listed as visible.\n",
        "    within_dist = np.sum(\n",
        "        np.square(pred_tracks - gt_tracks),\n",
        "        axis=-1,\n",
        "    ) < np.square(thresh)\n",
        "    is_correct = np.logical_and(within_dist, visible)\n",
        "\n",
        "    # Compute the frac_within_threshold, which is the fraction of points\n",
        "    # within the threshold among points that are visible in the ground truth,\n",
        "    # ignoring whether they're predicted to be visible.\n",
        "    count_correct = np.sum(\n",
        "        is_correct & evaluation_points,\n",
        "        axis=(1, 2),\n",
        "    )\n",
        "    count_visible_points = np.sum(visible & evaluation_points, axis=(1, 2))\n",
        "    frac_correct = count_correct / count_visible_points\n",
        "    metrics['pts_within_' + str(thresh)] = frac_correct\n",
        "    all_frac_within.append(frac_correct)\n",
        "\n",
        "    true_positives = np.sum(\n",
        "        is_correct & pred_visible & evaluation_points, axis=(1, 2)\n",
        "    )\n",
        "\n",
        "    # The denominator of the jaccard metric is the true positives plus\n",
        "    # false positives plus false negatives.  However, note that true positives\n",
        "    # plus false negatives is simply the number of points in the ground truth\n",
        "    # which is easier to compute than trying to compute all three quantities.\n",
        "    # Thus we just add the number of points in the ground truth to the number\n",
        "    # of false positives.\n",
        "    #\n",
        "    # False positives are simply points that are predicted to be visible,\n",
        "    # but the ground truth is not visible or too far from the prediction.\n",
        "    gt_positives = np.sum(visible & evaluation_points, axis=(1, 2))\n",
        "    false_positives = (~visible) & pred_visible\n",
        "    false_positives = false_positives | ((~within_dist) & pred_visible)\n",
        "    false_positives = np.sum(false_positives & evaluation_points, axis=(1, 2))\n",
        "    jaccard = true_positives / (gt_positives + false_positives)\n",
        "    metrics['jaccard_' + str(thresh)] = jaccard\n",
        "    all_jaccard.append(jaccard)\n",
        "  metrics['average_jaccard'] = np.mean(\n",
        "      np.stack(all_jaccard, axis=1),\n",
        "      axis=1,\n",
        "  )\n",
        "  metrics['average_pts_within_thresh'] = np.mean(\n",
        "      np.stack(all_frac_within, axis=1),\n",
        "      axis=1,\n",
        "  )\n",
        "  return metrics\n",
        "\n",
        "\n",
        "def evaluate(results: Dict[str, Any], label_dict: Dict[str, Any],\n",
        "                scale: float = 256.0) -> float:\n",
        "  \"\"\"Calculates the Average Jaccard for each video in the results.\n",
        "\n",
        "  Args:\n",
        "    results: A dictionary containing the results for each video.\n",
        "      The keys are video IDs, and the values are dictionaries with\n",
        "      'point_tracking' information.\n",
        "    label_dict: A dictionary containing the ground truth labels for each video.\n",
        "      The keys are video IDs, and the values are dictionaries with\n",
        "      'point_tracking' information.\n",
        "    scale: A float value representing the scaling factor (default is 256.0).\n",
        "\n",
        "  Returns:\n",
        "    The average Jaccard across all videos.\n",
        "\n",
        "  Raises:\n",
        "    AssertionError: If the lengths of predicted tracks and ground truth tracks\n",
        "      do not match.\n",
        "\n",
        "  \"\"\"\n",
        "  avg_jacs = []\n",
        "  static_avg_jacs = []\n",
        "  moving_avg_jacs = []\n",
        "  for video_id, video_item in results.items():\n",
        "    pred_tracks = video_item['point_tracking']\n",
        "    gt_tracks = label_dict[video_id]['point_tracking']\n",
        "    num_frames = label_dict[video_id]['metadata']['num_frames']\n",
        "    assert len(pred_tracks) == len(gt_tracks)\n",
        "    num_tracks = len(pred_tracks)\n",
        "\n",
        "    query_points = np.zeros((1, num_tracks, 3))\n",
        "    gt_occluded = np.ones((1, num_tracks, num_frames))\n",
        "    gt_points = np.zeros((1, num_tracks, num_frames, 2))\n",
        "    pred_occluded = np.ones((1, num_tracks, num_frames))\n",
        "    pred_points = np.zeros((1, num_tracks, num_frames, 2))\n",
        "\n",
        "    for track_idx, pred_track in enumerate(pred_tracks):\n",
        "      gt_track = gt_tracks[pred_track['id']]\n",
        "      gt_track_points = np.array(gt_track['points']).T\n",
        "      pred_track_points = np.array(pred_track['points']).T\n",
        "\n",
        "      start_point = gt_track_points[0]\n",
        "      start_frame_id = gt_track['frame_ids'][0]\n",
        "      query_points[0, track_idx, 0] = start_frame_id\n",
        "      query_points[0, track_idx, 1:] = start_point\n",
        "      gt_occluded[0, track_idx][gt_track['frame_ids']] = 0\n",
        "      pred_occluded[0, track_idx] = 0\n",
        "\n",
        "      gt_points[0, track_idx][gt_track['frame_ids']] = gt_track_points\n",
        "      pred_points[0, track_idx, :, :][pred_track['frame_ids']] = (\n",
        "          pred_track_points\n",
        "      )\n",
        "\n",
        "    gt_points *= scale\n",
        "    pred_points *= scale\n",
        "\n",
        "    metrics = compute_tapvid_metrics(query_points, gt_occluded,\n",
        "                                     gt_points, pred_occluded,\n",
        "                                     pred_points, 'first')\n",
        "    avg_jacs.append(metrics['average_jaccard'])\n",
        "\n",
        "    if label_dict[video_id]['metadata']['is_camera_moving']:\n",
        "      moving_avg_jacs.append(metrics['average_jaccard'])\n",
        "    else:\n",
        "      static_avg_jacs.append(metrics['average_jaccard'])\n",
        "\n",
        "  average_jaccard = np.mean(avg_jacs)\n",
        "  static_average_jaccard = np.mean(static_avg_jacs)\n",
        "  moving_average_jaccard = np.mean(moving_avg_jacs)\n",
        "  print(f'Average Jaccard across all videos: {average_jaccard}')\n",
        "  print(f'Average Jaccard across static videos: {static_average_jaccard}')\n",
        "  print(f'Average Jaccard across moving videos: {moving_average_jaccard}')\n",
        "  return average_jaccard"
      ],
      "metadata": {
        "id": "ZyChqk-hS3xe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Evaluate static baseline\n",
        "label_path = './data/point_tracking_valid.json'\n",
        "cfg = {'video_folder_path': './data/videos/',\n",
        "       'task': 'point_tracking',\n",
        "       'split': 'valid'}\n",
        "\n",
        "# init dataset\n",
        "tracking_dataset = PerceptionDataset(label_path, **cfg)\n",
        "\n",
        "# init tracking model\n",
        "point_tracker = PointTracker()\n",
        "\n",
        "# run model across full dataset\n",
        "results = {}\n",
        "for video_item in tracking_dataset:\n",
        "  video_id = video_item['metadata']['video_id']\n",
        "  video_pred_tracks = []\n",
        "  for gt_track in video_item['point_tracking']:\n",
        "    points = np.array(gt_track['points']).T\n",
        "    start_point = points[0]\n",
        "    start_frame_id = gt_track['frame_ids'][0]\n",
        "    pred_points, pred_frame_ids = (\n",
        "        point_tracker.track(video_item['frames'], start_point,\n",
        "                            start_frame_id)\n",
        "    )\n",
        "\n",
        "    pred_track = {}\n",
        "    # .tolist() to serialise without error\n",
        "    pred_track['points'] = pred_points.T.tolist()\n",
        "    pred_track['frame_ids'] = pred_frame_ids.tolist()\n",
        "    pred_track['id'] = gt_track['id']\n",
        "    video_pred_tracks.append(pred_track)\n",
        "\n",
        "  results[video_id] = {'point_tracking': video_pred_tracks}"
      ],
      "metadata": {
        "id": "0Yn588DzlhUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Serialise example results file\n",
        "# Writing model outputs in the expected competition format. This\n",
        "# JSON file contains answers to all questions in the validation split in the\n",
        "# format:\n",
        "\n",
        "# example_submission = {'video_1009' :{'point_tracking':[\n",
        "#     {'id': 0, 'points': [[y,x],...], 'frame_ids': [n,...]},\n",
        "#     {'id': 1, 'points': [[y,x],...], 'frame_ids': [n,...]},...]}}\n",
        "\n",
        "# This file could be used directly as a submission for the Eval.ai challenge\n",
        "with open(f'{cfg[\"task\"]}_{cfg[\"split\"]}_results.json', 'w') as my_file:\n",
        "  json.dump(results, my_file)"
      ],
      "metadata": {
        "id": "h_dEcM__pV90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Compute average Jaccard\n",
        "label_dict = load_db_json(label_path)\n",
        "aj = evaluate(results, label_dict)"
      ],
      "metadata": {
        "id": "enFk726JX_-c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95d7f3de-15dc-4d59-a8a5-e4d91a6ea65f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Jaccard across all videos: 0.3606894073177557\n",
            "Average Jaccard across static videos: 0.4092416410388438\n",
            "Average Jaccard across moving videos: 0.08703136270798537\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Visualisation functions\n",
        "def get_colors(num_colors: int) -> Tuple[int, int, int]:\n",
        "  \"\"\"Generate random colormaps for visualizing different objects and points.\n",
        "\n",
        "  Args:\n",
        "    num_colors (int): The number of colors to generate.\n",
        "\n",
        "  Returns:\n",
        "    Tuple[int, int, int]: A tuple of RGB values representing the\n",
        "      generated colors.\n",
        "  \"\"\"\n",
        "  colors = []\n",
        "  for j in np.arange(0., 360., 360. / num_colors):\n",
        "    hue = j / 360.\n",
        "    lightness = (50 + np.random.rand() * 10) / 100.\n",
        "    saturation = (90 + np.random.rand() * 10) / 100.\n",
        "    color = colorsys.hls_to_rgb(hue, lightness, saturation)\n",
        "    color = (int(color[0] * 255), int(color[1] * 255), int(color[2] * 255))\n",
        "    colors.append(color)\n",
        "  random.seed(0)\n",
        "  random.shuffle(colors)\n",
        "  return colors\n",
        "\n",
        "\n",
        "def display_video(vid_frames: np.array, fps: int = 30):\n",
        "  \"\"\"Create and display temporary video from numpy array frames.\n",
        "\n",
        "  Args:\n",
        "    vid_frames: (np.array): The frames of the video as a\n",
        "    \tnumpy array. Format of frames should be:\n",
        "    \t(num_frames, height, width, channels)\n",
        "    fps (int): Frames per second for the video playback. Default is 30.\n",
        "  \"\"\"\n",
        "  kwargs = {'macro_block_size': None}\n",
        "  imageio.mimwrite('tmp_video_display.mp4',\n",
        "                   vid_frames[:, :, :, ::-1], fps=fps, **kwargs)\n",
        "  display(mvp.ipython_display('tmp_video_display.mp4'))\n",
        "\n",
        "\n",
        "def paint_point(video: np.array, track: dict,\n",
        "                color: tuple[int, int, int] = (255, 0, 0),\n",
        "                label: str = None) -> np.array:\n",
        "  \"\"\"Paints a single tracked point on each frame of a video.\n",
        "\n",
        "  Args:\n",
        "    video (np.array): The video frames as a numpy array.\n",
        "    track (dict): The track containing frame IDs and corresponding points.\n",
        "    color (tuple, optional): The color of the painted point.\n",
        "      Defaults to (255, 0, 0).\n",
        "    label (str): string to be added to point label annotation.\n",
        "\n",
        "  Returns:\n",
        "    np.array: The video frames with painted points.\n",
        "  \"\"\"\n",
        "  _, height, width, _ = video.shape\n",
        "  for idx, frame_id in enumerate(track['frame_ids']):\n",
        "    vid_frame = video[frame_id]\n",
        "    y = int(round(track['points'][0][idx] * height))\n",
        "    x = int(round(track['points'][1][idx] * width))\n",
        "    vid_frame = cv2.circle(vid_frame, (x, y), radius=10,\n",
        "                           color=color, thickness=-1)\n",
        "    if label is not None:\n",
        "      vid_frame = cv2.putText(vid_frame, label, (x, y + 20),\n",
        "                              cv2.FONT_HERSHEY_SIMPLEX, 0.75, color, 2)\n",
        "    video[frame_id] = vid_frame\n",
        "  return video\n",
        "\n",
        "\n",
        "def paint_points(video: np.array, tracks: List[dict]) -> np.array:\n",
        "  \"\"\"Paints multiple tracked points on each frame of a video.\n",
        "\n",
        "  Args:\n",
        "    video (np.array): The video frames as a numpy array.\n",
        "    tracks (List[dict]): The list of tracks containing\n",
        "      frame IDs and corresponding points.\n",
        "\n",
        "  Returns:\n",
        "    np.array: The video frames with painted points.\n",
        "  \"\"\"\n",
        "  for idx, track in enumerate(tracks):\n",
        "    video = paint_point(video, track, COLORS[idx])\n",
        "  return video"
      ],
      "metadata": {
        "id": "dafGHrzlPnBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Show Example Annotations\n",
        "COLORS = get_colors(num_colors=100)\n",
        "\n",
        "db_dict = load_db_json(db_json_path)\n",
        "\n",
        "video_id = list(db_dict.keys())[6]\n",
        "example_data = db_dict[video_id]\n",
        "\n",
        "if example_data['point_tracking']:\n",
        "  frames = get_video_frames(example_data, video_path)\n",
        "  tracks_to_show = 0.5  # @param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "  num_tracks = int(len(example_data['point_tracking']) * tracks_to_show)\n",
        "  frames = paint_points(frames, example_data['point_tracking'][0:num_tracks])\n",
        "  display_video(frames, example_data['metadata']['frame_rate'])\n",
        "  del frames"
      ],
      "metadata": {
        "id": "hPJkHKWPEvff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Model outputs visualised (static points)\n",
        "# here we show how actual inference would work with video frames loaded\n",
        "if example_data['point_tracking']:\n",
        "  get_video_frames(example_data, video_path)\n",
        "  frames = get_video_frames(example_data, video_path)\n",
        "\n",
        "  pred_tracks = []\n",
        "  for gt_track in example_data['point_tracking']:\n",
        "    points = np.array(gt_track['points']).T\n",
        "    start_point = points[0]\n",
        "    start_frame_id = gt_track['frame_ids'][0]\n",
        "    pred_points, pred_frame_ids = (\n",
        "        point_tracker.track(frames, start_point,\n",
        "                            start_frame_id)\n",
        "    )\n",
        "    pred_track = {}\n",
        "    pred_track['points'] = pred_points.T.tolist()\n",
        "    pred_track['frame_ids'] = pred_frame_ids.tolist()\n",
        "    pred_track['label'] = gt_track['label']\n",
        "    pred_track['id'] = gt_track['id']\n",
        "    pred_tracks.append(pred_track)\n",
        "\n",
        "  tracks_to_show = 0.5  # @param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "  num_tracks = int(len(pred_tracks) * tracks_to_show)\n",
        "  frames = paint_points(frames, pred_tracks[0: num_tracks])\n",
        "  display_video(frames, video_item['metadata']['frame_rate'])\n",
        "  del frames"
      ],
      "metadata": {
        "id": "oVFlyvJFNW-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Comparing ground truth labels vs model outputs\n",
        "if example_data['point_tracking']:\n",
        "\n",
        "  track_to_compare = 1  # @param {type:\"integer\"}\n",
        "  if track_to_compare > len(example_data['point_tracking']):\n",
        "    raise ValueError(f'Track {track_to_compare} is not in the video')\n",
        "\n",
        "  frames = get_video_frames(example_data, video_path)\n",
        "  frames = paint_point(frames, example_data['point_tracking'][track_to_compare],\n",
        "                    color=COLORS[0], label=': gt')\n",
        "  frames = paint_point(frames, pred_tracks[track_to_compare], color=COLORS[1],\n",
        "                    label=': pred')\n",
        "\n",
        "  display_video(frames, video_item['metadata']['frame_rate'])\n",
        "  del frames"
      ],
      "metadata": {
        "id": "cKC8IGOjWbRf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
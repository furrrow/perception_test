{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepmind/perception_test/blob/main/baselines/grounded_vqa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook demonstrates how to load the grounded video question answering annotations in the validation split of the Perception Test and run the evaluation for a simple baseline model, which first runs MDETR on the middle frame of a video conditioned on the question, and then assumes static boxes throughout the video."
      ],
      "metadata": {
        "id": "JykaDwpmWOcu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copyright 2023 DeepMind Technologies Limited\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at [https://www.apache.org/licenses/LICENSE-2.0](https://www.apache.org/licenses/LICENSE-2.0).\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n"
      ],
      "metadata": {
        "id": "Ov2zD0apsX3A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grounded Video Question Answering Baseline for the Perception Test\n",
        "\n",
        "Github: https://github.com/deepmind/perception_test\n",
        "\n",
        "## The Perception Test\n",
        "[Perception Test: A Diagnostic Benchmark for Multimodal Video Models](https://arxiv.org/abs/2305.13786) is a multimodal benchmark designed to comprehensively evaluate the perception and reasoning skills of multimodal video models. The Perception Test dataset introduces real-world videos designed to show perceptually interesting situations and defines multiple computational tasks (object and point tracking, action and sound localisation, multiple-choice and grounded video question-answering). Here, we provide details and a simple baseline for the grounded video question-answering task.\n",
        "\n",
        "[![Perception Test Overview Presentation](https://img.youtube.com/vi/8BiajMOBWdk/maxresdefault.jpg)](https://youtu.be/8BiajMOBWdk?t=10)"
      ],
      "metadata": {
        "id": "NvbIw-q_sYm3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Grounded Video Question Answering\n",
        "This task is similar to conditional multiple-object tracking,\n",
        "with the conditioning given as a language task or question as opposed to a class label. The answers are object tracks defined throughout the video.\n",
        "\n",
        "The below image shows an example of video-question-answer tuple for the grounded video QA task.\n",
        "\n",
        "![image collage](https://storage.googleapis.com/dm-perception-test/img/gqa.png)"
      ],
      "metadata": {
        "id": "Mgn0hN2pLktg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MDETR-Static baseline\n",
        "This notebook demonstrates how to load the annotations in the validation split of the Perception Test, and run the evaluation for a simple model where the objects to track are detected by MDETR on the middle frame of the video, conditioned by the question. The detections are then kept static throughout the video."
      ],
      "metadata": {
        "id": "SNJIaORcLnHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Setup and install\n",
        "\n",
        "! pip install timm transformers\n",
        "\n",
        "!git clone --recursive https://github.com/facebookresearch/multimodal.git multimodal\n",
        "%cd multimodal\n",
        "!pip install -e .\n",
        "%cd ..\n",
        "!cp -r /content/multimodal/examples/mdetr/data .\n",
        "!cp -r /content/multimodal/examples/mdetr/utils .\n",
        "\n",
        "# VOT toolkit required for evaluation\n",
        "!pip install git+https://github.com/votchallenge/vot-toolkit-python\n",
        "\n",
        "# TrackEval required for HOTA\n",
        "!git clone https://github.com/JonathonLuiten/TrackEval.git\n",
        "%cd TrackEval\n",
        "!pip install -e .\n",
        "%cd ..\n",
        "\n",
        "# RESTART RUNTIME AFTER THESE COMMANDS"
      ],
      "metadata": {
        "id": "VlEOnxHVeoiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Up7CEnJriAVu"
      },
      "outputs": [],
      "source": [
        "# @title Prerequisites\n",
        "import collections\n",
        "import colorsys\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "from typing import Tuple, List, Dict, Any\n",
        "import zipfile\n",
        "\n",
        "import cv2\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import moviepy.editor as mvp\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "from torchmultimodal.models.mdetr.model import mdetr_for_vqa\n",
        "from torchvision.ops.boxes import box_convert\n",
        "import torchvision.transforms as T\n",
        "from trackeval.metrics import HOTA\n",
        "from transformers import RobertaTokenizerFast"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Utility functions\n",
        "def download_and_unzip(url: str, destination: str):\n",
        "  \"\"\"Downloads and unzips a .zip file to a destination.\n",
        "\n",
        "  Downloads a file from the specified URL, saves it to the destination\n",
        "  directory, and then extracts its contents.\n",
        "\n",
        "  If the file is larger than 1GB, it will be downloaded in chunks,\n",
        "  and the download progress will be displayed.\n",
        "\n",
        "  Args:\n",
        "    url (str): The URL of the file to download.\n",
        "    destination (str): The destination directory to save the file and\n",
        "      extract its contents.\n",
        "  \"\"\"\n",
        "  if not os.path.exists(destination):\n",
        "    os.makedirs(destination)\n",
        "\n",
        "  filename = url.split('/')[-1]\n",
        "  file_path = os.path.join(destination, filename)\n",
        "\n",
        "  if os.path.exists(file_path):\n",
        "    print(f'{filename} already exists. Skipping download.')\n",
        "    return\n",
        "\n",
        "  response = requests.get(url, stream=True)\n",
        "  total_size = int(response.headers.get('content-length', 0))\n",
        "  gb = 1024*1024*1024\n",
        "\n",
        "  if total_size / gb > 1:\n",
        "    print(f'{filename} is larger than 1GB, downloading in chunks')\n",
        "    chunk_flag = True\n",
        "    chunk_size = int(total_size/100)\n",
        "  else:\n",
        "    chunk_flag = False\n",
        "    chunk_size = total_size\n",
        "\n",
        "  with open(file_path, 'wb') as file:\n",
        "    for chunk_idx, chunk in enumerate(\n",
        "        response.iter_content(chunk_size=chunk_size)):\n",
        "      if chunk:\n",
        "        if chunk_flag:\n",
        "          print(f\"\"\"{chunk_idx}% downloading\n",
        "          {round((chunk_idx*chunk_size)/gb, 1)}GB\n",
        "          / {round(total_size/gb, 1)}GB\"\"\")\n",
        "        file.write(chunk)\n",
        "  print(f\"'{filename}' downloaded successfully.\")\n",
        "\n",
        "  with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(destination)\n",
        "  print(f\"'{filename}' extracted successfully.\")\n",
        "\n",
        "  os.remove(file_path)\n",
        "\n",
        "\n",
        "def load_db_json(db_file: str) -> Dict[str, Any]:\n",
        "  \"\"\"Loads a JSON file as a dictionary.\n",
        "\n",
        "  Args:\n",
        "    db_file (str): Path to the JSON file.\n",
        "\n",
        "  Returns:\n",
        "    Dict: Loaded JSON data as a dictionary.\n",
        "\n",
        "  Raises:\n",
        "    FileNotFoundError: If the specified file doesn't exist.\n",
        "    TypeError: If the JSON file is not formatted as a dictionary.\n",
        "  \"\"\"\n",
        "  if not os.path.isfile(db_file):\n",
        "    raise FileNotFoundError(f'No such file: {db_file}')\n",
        "\n",
        "  with open(db_file, 'r') as f:\n",
        "    db_file_dict = json.load(f)\n",
        "    if not isinstance(db_file_dict, dict):\n",
        "      raise TypeError('JSON file is not formatted as a dictionary.')\n",
        "    return db_file_dict\n",
        "\n",
        "\n",
        "def load_mp4_to_frames(filename: str) -> np.array:\n",
        "  \"\"\"Loads an MP4 video file and returns its frames as a NumPy array.\n",
        "\n",
        "  Args:\n",
        "    filename (str): Path to the MP4 video file.\n",
        "\n",
        "  Returns:\n",
        "    np.array: Frames of the video as a NumPy array.\n",
        "  \"\"\"\n",
        "  assert os.path.exists(filename), f'File {filename} does not exist.'\n",
        "  cap = cv2.VideoCapture(filename)\n",
        "\n",
        "  num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "  height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "  width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "\n",
        "  vid_frames = np.empty((num_frames, height, width, 3), dtype=np.uint8)\n",
        "\n",
        "  idx = 0\n",
        "  while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "      break\n",
        "\n",
        "    vid_frames[idx] = frame\n",
        "    idx += 1\n",
        "\n",
        "  cap.release()\n",
        "  return vid_frames\n",
        "\n",
        "\n",
        "def get_video_frames(data_item: Dict[str, Any], video_path: str) -> np.array:\n",
        "  \"\"\"Loads frames of a video specified by an item dictionary.\n",
        "\n",
        "  Assumes format of annotations used in the Perception Test Dataset.\n",
        "\n",
        "  Args:\n",
        "  \tdata_item (Dict): Item from dataset containing metadata.\n",
        "    video_path (str): Path to the directory containing videos.\n",
        "\n",
        "  Returns:\n",
        "    np.array: Frames of the video as a NumPy array.\n",
        "  \"\"\"\n",
        "  video_file_path = os.path.join(video_path,\n",
        "                                 data_item['metadata']['video_id']) + '.mp4'\n",
        "  vid_frames = load_mp4_to_frames(video_file_path)\n",
        "  assert data_item['metadata']['num_frames'] == vid_frames.shape[0]\n",
        "  return vid_frames\n",
        "\n",
        "\n",
        "def load_single_frame(filename: str, frame_idx) -> np.array:\n",
        "  \"\"\"Loads an MP4 video file and returns a single frame as a NumPy array.\n",
        "\n",
        "  This function loads a specific frame from the given MP4 video file and returns\n",
        "  it as a NumPy array.\n",
        "\n",
        "  Args:\n",
        "    filename (str): Path to the MP4 video file.\n",
        "    frame_idx (int): The index of the frame to be loaded.\n",
        "\n",
        "  Returns:\n",
        "    np.array: A single frame of the video as a NumPy array.\n",
        "\n",
        "  Raises:\n",
        "    AssertionError: If the given file does not exist.\n",
        "  \"\"\"\n",
        "  assert os.path.exists(filename), f'File {filename} does not exist.'\n",
        "  cap = cv2.VideoCapture(filename)\n",
        "  cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "  _, frame = cap.read()\n",
        "  cap.release()\n",
        "  return frame"
      ],
      "metadata": {
        "id": "1q_JZ0ueikxp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # @title Download data\n",
        "data_path = './data/'\n",
        "\n",
        "split = 'valid'  # @param ['sample', 'valid']\n",
        "\n",
        "if split == 'valid':\n",
        "  # full validaton set - takes approx 30 mins to download\n",
        "  valid_annot_url = 'https://storage.googleapis.com/dm-perception-test/zip_data/grounded_question_valid_annotations.zip'\n",
        "  download_and_unzip(valid_annot_url, data_path)\n",
        "  valid_videos_url = 'https://storage.googleapis.com/dm-perception-test/zip_data/grounded_question_valid_videos.zip'\n",
        "  download_and_unzip(valid_videos_url, data_path)\n",
        "\n",
        "elif split == 'sample':\n",
        "  # sample annotations and videos (small subset for demo)\n",
        "  sample_annot_url = 'https://storage.googleapis.com/dm-perception-test/zip_data/sample_annotations.zip'\n",
        "  download_and_unzip(sample_annot_url, data_path)\n",
        "  sample_videos_url = 'https://storage.googleapis.com/dm-perception-test/zip_data/sample_videos.zip'\n",
        "  download_and_unzip(sample_videos_url, data_path)"
      ],
      "metadata": {
        "id": "on4M6kzSwQ22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Dataset class\n",
        "class PerceptionGQADataset(Dataset):\n",
        "  \"\"\"Dataset class to store video items from dataset.\n",
        "\n",
        "  Attributes:\n",
        "    video_folder_path: Path to the folder containing the videos.\n",
        "    task: Task type for annotations.\n",
        "    split: Dataset split to load.\n",
        "  \ttask_db: List containing annotations for dataset according to\n",
        "  \t\tsplit and task availability.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, db_path: Dict[str, Any], video_folder_path: str,\n",
        "               task: str, split: str) -> None:\n",
        "    \"\"\"Initializes the PerceptionDataset class.\n",
        "\n",
        "    Args:\n",
        "      db_path (str): Path to the annotation file.\n",
        "      video_folder_path (str): Path to the folder containing the videos.\n",
        "      task (str): Task type for annotations.\n",
        "      split (str): Dataset split to load.\n",
        "    \"\"\"\n",
        "    self.video_folder_path = video_folder_path\n",
        "    self.task = task\n",
        "    self.split = split\n",
        "    self.task_db = self.load_dataset(db_path)\n",
        "\n",
        "  def load_dataset(self, db_path: str) -> List:\n",
        "    \"\"\"Loads the dataset from the annotation file and processes.\n",
        "\n",
        "    Dict is processed according to split and task.\n",
        "\n",
        "    Args:\n",
        "      db_path (str): Path to the annotation file.\n",
        "\n",
        "    Returns:\n",
        "      List: List of database items containing annotations.\n",
        "    \"\"\"\n",
        "    db_dict = load_db_json(db_path)\n",
        "    db_list = []\n",
        "    for _, val in db_dict.items():\n",
        "      if val['metadata']['split'] == self.split:\n",
        "        if val[self.task]:  # If video has annotations for this task\n",
        "          db_list.append(val)\n",
        "\n",
        "    return db_list\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    \"\"\"Returns the total number of videos in the dataset.\n",
        "\n",
        "    Returns:\n",
        "      int: Total number of videos.\n",
        "    \"\"\"\n",
        "    return len(self.pt_db_list)\n",
        "\n",
        "  def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
        "    \"\"\"Returns the video and annotations for a given index.\n",
        "\n",
        "    Args:\n",
        "      idx (int): Index of the video.\n",
        "\n",
        "    Returns:\n",
        "      Dict: Dictionary containing the video frames, metadata, annotations.\n",
        "    \"\"\"\n",
        "    data_item = self.task_db[idx]\n",
        "    annot = data_item[self.task]\n",
        "\n",
        "    metadata = data_item['metadata']\n",
        "    frame_idx = round(data_item['metadata']['num_frames']/2)\n",
        "    video_file_path = os.path.join(self.video_folder_path,\n",
        "                                   data_item['metadata']['video_id']) + '.mp4'\n",
        "    frame = load_single_frame(video_file_path, frame_idx)\n",
        "    frame = torch.tensor(frame[:,:,::-1].copy())\n",
        "    frame = frame.permute(2, 0, 1)[None, ...].float()/255.\n",
        "\n",
        "    vid_frames = np.zeros((metadata['num_frames'], 1, 1, 1))\n",
        "\n",
        "    return {'metadata': metadata,\n",
        "            'grounded_question': annot,\n",
        "            'object_tracking': data_item['object_tracking'],\n",
        "            'vqa_frame': frame,\n",
        "            'frames': vid_frames}"
      ],
      "metadata": {
        "id": "cA7ehjrKgeHr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Object tracking model (static)\n",
        "class ObjectTracker():\n",
        "  \"\"\"Object tracker class that tracks a given object in a video.\n",
        "\n",
        "  This model assumes static boxes, given the first bounding box\n",
        "  which should be tracked in a sequence, for every frame in the\n",
        "  remaining sequence it will return the same coordinates.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    \"\"\"Initializes the ObjectTracker class.\"\"\"\n",
        "    pass\n",
        "\n",
        "  def track_object_in_video(self, frames: np.array, start_info: Dict[str, Any]\n",
        "                            )-> Dict[str, Any]:\n",
        "    \"\"\"Tracks an object in a video.\n",
        "\n",
        "    Tracks an object given a sequence of frames and initial information about\n",
        "    the coordinates and frame ID of the object.\n",
        "\n",
        "    Args:\n",
        "      frames (np.array): Array of frames representing the video.\n",
        "      start_info (Dict): Dictionary containing the start bounding box and\n",
        "        frame ID.\n",
        "\n",
        "    Returns:\n",
        "      Dict[str: List]: Dictionary containing the tracked bounding boxes and\n",
        "        corresponding frame IDs.\n",
        "    \"\"\"\n",
        "    # initially take starting bounding box for tracking\n",
        "    prev_bb = start_info['start_bounding_box']\n",
        "    output_bounding_boxes = []\n",
        "    output_frame_ids = []\n",
        "\n",
        "    for frame_id in range(start_info['start_id'], frames.shape[0]):\n",
        "      frame = frames[frame_id]\n",
        "      # here is where the per frame tracking is done by the model\n",
        "      # we just return the starting coords in this dummy baseline\n",
        "      bb = self.track_object_in_frame(frame, prev_bb)\n",
        "      output_bounding_boxes.append(bb)\n",
        "      output_frame_ids.append(frame_id)\n",
        "\n",
        "    output_bounding_boxes = np.stack(output_bounding_boxes, axis=0)\n",
        "    output_frame_ids = np.array(output_frame_ids)\n",
        "    return output_bounding_boxes, output_frame_ids\n",
        "\n",
        "  # model inference would be inserted here!!\n",
        "  def track_object_in_frame(self, frame: np.array,\n",
        "                            prev_bb: List[float]) -> List[float]:\n",
        "    \"\"\"Tracks an object in a single frame.\n",
        "\n",
        "    Tracks an object in a single frame based on the previous bounding box\n",
        "    coordinates. Placeholder function that just returns the coords it is given,\n",
        "    assumes a static object.\n",
        "\n",
        "    Args:\n",
        "      frame (np.array): The current frame.\n",
        "      prev_bb(List): Previous bounding box coordinates. (y2,x2,y1,x1)\n",
        "\n",
        "    Returns:\n",
        "      List: The tracked bounding box coordinates in the current frame.\n",
        "    \"\"\"\n",
        "    del frame  # unused\n",
        "    return prev_bb"
      ],
      "metadata": {
        "id": "192UyZxbBbra"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Evaluation functions\n",
        "def get_start_frame(track_arr: List[List[float]]) -> int:\n",
        "  \"\"\"Returns index of the first non-zero element in a track array.\n",
        "\n",
        "  Args:\n",
        "    track_arr (list): one hot vector correspoinding to annotations,\n",
        "      showing which index to start tracking .\n",
        "\n",
        "  Returns:\n",
        "    int: Index of the first non-zero element in the track array.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: Raises error if the length of the array is 0\n",
        "      or if there is no one-hot value.\n",
        "  \"\"\"\n",
        "  if not track_arr or np.count_nonzero(track_arr) == 0:\n",
        "    raise ValueError('Track is empty or has no non-zero elements')\n",
        "  return np.nonzero(track_arr)[0][0]\n",
        "\n",
        "\n",
        "def get_start_info(track: Dict[str, Any]) -> Dict[str, Any]:\n",
        "  \"\"\"Retrieve information about the start frame of a track.\n",
        "\n",
        "  Args:\n",
        "    track (Dict): A dictionary containing information about the track.\n",
        "\n",
        "  Returns:\n",
        "    Dict[str: Any]: A dictionary with the following keys:\n",
        "      'start_id': The frame ID of the start frame.\n",
        "      'start_bounding_box': The bounding box coordinates of the start\n",
        "        frame.\n",
        "      'start_idx': The index of the start frame in the\n",
        "      'bounding_boxes' list.\n",
        "  \"\"\"\n",
        "  track_start_idx = get_start_frame(track['initial_tracking_box'])\n",
        "  track_start_id = track['frame_ids'][track_start_idx]\n",
        "  track_start_bb = track['bounding_boxes'][track_start_idx]\n",
        "\n",
        "  return {'start_id': track_start_id,\n",
        "          'start_bounding_box': track_start_bb,\n",
        "          'start_idx': track_start_idx}\n",
        "\n",
        "\n",
        "def filter_pred_boxes(pred_bb: np.ndarray, pred_fid: np.ndarray,\n",
        "                      gt_fid: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
        "  \"\"\"Filter bounding boxes and frame IDs based on ground truth frame IDs.\n",
        "\n",
        "  Args:\n",
        "    pred_bb (np.ndarray): Array of predicted bounding boxes.\n",
        "    pred_fid (np.ndarray): Array of frame IDs for predicted bounding boxes.\n",
        "    gt_fid (np.ndarray): Array of frame IDs for ground truth bounding boxes.\n",
        "\n",
        "  Returns:\n",
        "    Tuple[np.ndarray, np.ndarray]: Filtered predicted bounding boxes and\n",
        "    \ttheir corresponding frame IDs.\n",
        "  \"\"\"\n",
        "  pred_idx = np.isin(pred_fid, gt_fid).nonzero()[0]\n",
        "  filter_pred_bb = pred_bb[pred_idx]\n",
        "  filter_pred_fid = pred_fid[pred_idx]\n",
        "  return filter_pred_bb, filter_pred_fid\n",
        "\n",
        "\n",
        "def rescale_tensor_boxes(boxes: torch.Tensor,\n",
        "                         size: Tuple[int, int]) -> torch.Tensor:\n",
        "  \"\"\"Rescales predicted boxes to match the image size.\n",
        "\n",
        "  Args:\n",
        "    boxes (torch.Tensor): Tensor of bounding boxes in the format\n",
        "      \"cxcywh\" (center x, center y, width, height).\n",
        "    size (Tuple): A tuple representing the height and width of the\n",
        "      image.\n",
        "\n",
        "  Returns:\n",
        "    torch.Tensor: Tensor of rescaled bounding boxes in the format \"xyxy\"\n",
        "      (xmin, ymin, xmax, ymax).\n",
        "  \"\"\"\n",
        "  img_h, img_w = size\n",
        "  b = box_convert(boxes, 'cxcywh', 'xyxy')\n",
        "  b = b * torch.tensor([img_w, img_h, img_w, img_h],\n",
        "                       dtype=torch.float32).to(DEVICE)\n",
        "  return b\n",
        "\n",
        "\n",
        "def rescale_list_boxes(box: List[float], size: Tuple[int, int]) -> List[float]:\n",
        "  \"\"\"Rescales a list bounding box to match the image size.\n",
        "\n",
        "  Args:\n",
        "    box (List): List of bounding box coordinates\n",
        "      [xmin, ymin, xmax, ymax].\n",
        "    size (Tuple): A tuple representing the height and width of the\n",
        "      image.\n",
        "\n",
        "  Returns:\n",
        "    List: List of rescaled bounding box coordinates\n",
        "      [xmin_rescaled, ymin_rescaled, xmax_rescaled, ymax_rescaled].\n",
        "  \"\"\"\n",
        "  [img_h,img_w] = size\n",
        "  return [box[0] / img_w, box[1] / img_h, box[2] / img_w, box[3] / img_h]\n",
        "\n",
        "\n",
        "def postprocess(outputs: Any, text: str, tokenized: Any,\n",
        "                         img_size: Tuple[int, int], keep_prob: float = 0.7\n",
        "                         ) -> Tuple[List[float], List[torch.Tensor], List[str]]:\n",
        "  \"\"\"Postprocessing to rescale boxes and extract predicted spans of tokens.\n",
        "\n",
        "  Args:\n",
        "    outputs (Any): The outputs from the model.\n",
        "    text (str): The original text of the question.\n",
        "    tokenized (Any): The tokenized version of the text.\n",
        "    img_size (Tuple): A tuple representing the height and width of\n",
        "      the image.\n",
        "    keep_prob (float, optional): Probability threshold for keeping predicted\n",
        "      boxes. Defaults to 0.7.\n",
        "\n",
        "  Returns:\n",
        "    Tuple[List[float], List[torch.Tensor], List[str]]: A tuple containing:\n",
        "        - List of probabilities for each predicted box.\n",
        "        - List of rescaled bounding boxes for each predicted box.\n",
        "        - List of labels for each predicted box.\n",
        "\n",
        "  Note:\n",
        "    The type of 'outputs' and 'tokenized' parameters should have specific\n",
        "      attributes and methods, as used in the function.\n",
        "  \"\"\"\n",
        "  model_outputs = outputs.model_output\n",
        "  probs = 1 - model_outputs.pred_logits.softmax(-1)[0, :, -1]\n",
        "  keep = (probs > keep_prob)\n",
        "\n",
        "  # convert boxes from [0; 1] to image scales\n",
        "  boxes_scaled = rescale_tensor_boxes(model_outputs.pred_boxes[0, keep],\n",
        "                                      img_size)\n",
        "\n",
        "  # Extract the text spans predicted by each box\n",
        "  positive_tokens = model_outputs.pred_logits[0, keep].softmax(-1) > 0.1\n",
        "  positive_tokens = positive_tokens.nonzero().tolist()\n",
        "  pred_spans = collections.defaultdict(str)\n",
        "  for tok in positive_tokens:\n",
        "    item, pos = tok\n",
        "    if pos < 255:\n",
        "      span = tokenized.token_to_chars(0, pos)\n",
        "      if span is not None:\n",
        "        pred_spans[item] += ' ' + text[span.start:span.end]\n",
        "\n",
        "  probs = probs[keep]\n",
        "  probs = [probs[int(k)] for k in sorted(list(pred_spans.keys()))]\n",
        "  boxes_scaled = [boxes_scaled[int(k)] for k in sorted(list(pred_spans.keys()))]\n",
        "  labels = [pred_spans[k] for k in sorted(list(pred_spans.keys()))]\n",
        "\n",
        "  return probs, boxes_scaled, labels\n",
        "\n",
        "\n",
        "def search_frame_ids(frame_id_dict: Dict[int, List[int]]\n",
        "                    ) -> Dict[int, Any]:\n",
        "  \"\"\"Search for frame IDs in tracks to get per Frame ID track ID dict.\n",
        "\n",
        "  Args:\n",
        "    frame_id_dict (Dict): A dictionary containing track IDs as\n",
        "      keys and lists of corresponding frame IDs as values.\n",
        "\n",
        "  Returns:\n",
        "    Dict[int, List]: A combined dictionary with frame IDs as keys and\n",
        "      lists of track IDs containing each frame ID as values.\n",
        "  \"\"\"\n",
        "  combined_id_dict = {}\n",
        "\n",
        "  for k, v in frame_id_dict.items():\n",
        "    for i in v:\n",
        "      if i not in combined_id_dict:\n",
        "        combined_id_dict[i] = [k]\n",
        "      else:\n",
        "        combined_id_dict[i].append(k)\n",
        "\n",
        "  return combined_id_dict\n",
        "\n",
        "\n",
        "def build_gt_ids(tracks: List[Dict[str, Any]], gt: bool,\n",
        "                 track_ids: List[int] = None) -> Dict[int, List[int]]:\n",
        "  \"\"\"Build ground truth track IDs dict based on input tracks and track IDs.\n",
        "\n",
        "  Args:\n",
        "    tracks (List): A list of track\n",
        "      dictionaries, each containing 'id' and 'frame_ids' as keys.\n",
        "    gt (bool): A boolean indicating if ground truth track IDs are to be built.\n",
        "    track_ids (List, optional): A list of track IDs to be considered.\n",
        "      If None, all tracks will be considered. Defaults to None.\n",
        "\n",
        "  Returns:\n",
        "    Dict: A dictionary containing track IDs as keys and lists\n",
        "      of corresponding frame IDs as values.\n",
        "  \"\"\"\n",
        "  track_dict = {}\n",
        "  if track_ids is None:\n",
        "    nid = 0\n",
        "    for track in tracks:\n",
        "      if gt:\n",
        "        start_info = get_start_info(track)\n",
        "        track_dict[nid] = track['frame_ids'][start_info['start_idx']:]\n",
        "      else:\n",
        "        track_dict[nid] = track['frame_ids']\n",
        "      nid += 1\n",
        "\n",
        "  else:\n",
        "    nid = 0\n",
        "    for t in track_ids:\n",
        "      track = tracks[t]\n",
        "      assert track['id'] == t\n",
        "      if gt:\n",
        "        start_info = get_start_info(track)\n",
        "        track_dict[nid] = track['frame_ids'][start_info['start_idx']:]\n",
        "      else:\n",
        "        track_dict[nid] = track['frame_ids']\n",
        "      nid += 1\n",
        "\n",
        "  frame_id_dict = search_frame_ids(track_dict)\n",
        "  return frame_id_dict\n",
        "\n",
        "\n",
        "def calculate_iou(boxes1: np.array, boxes2: np.array) -> float:\n",
        "  \"\"\"Calculate Intersection over Union (IoU) for two sets of bounding boxes.\n",
        "\n",
        "  Args:\n",
        "    boxes1 (np.array): Bounding boxes in the format [y2, x2, y1, x1],\n",
        "      shape (n, 4)\n",
        "    boxes2 (np.array): Bounding boxes in the format [y2, x2, y1, x1],\n",
        "      shape (n, 4)\n",
        "\n",
        "  Returns:\n",
        "    iou (float): Intersection over Union (IoU) float value\n",
        "  \"\"\"\n",
        "  x1_1, y1_1, x2_1, y2_1 = np.split(boxes1, 4, axis=1)\n",
        "  x1_2, y1_2, x2_2, y2_2 = np.split(boxes2, 4, axis=1)\n",
        "\n",
        "  # Find intersection coordinates\n",
        "  y1_inter = np.maximum(y1_1, y1_2)\n",
        "  x1_inter = np.maximum(x1_1, x1_2)\n",
        "  y2_inter = np.minimum(y2_1, y2_2)\n",
        "  x2_inter = np.minimum(x2_1, x2_2)\n",
        "\n",
        "  # Calculate area of intersection\n",
        "  h_inter = np.maximum(0, y2_inter - y1_inter)\n",
        "  w_inter = np.maximum(0, x2_inter - x1_inter)\n",
        "  area_inter = h_inter * w_inter\n",
        "\n",
        "  # Calculate area of union\n",
        "  area_boxes1 = (y2_1 - y1_1) * (x2_1 - x1_1)\n",
        "  area_boxes2 = (y2_2 - y1_2) * (x2_2 - x1_2)\n",
        "  union = area_boxes1 + area_boxes2 - area_inter\n",
        "\n",
        "  return area_inter / union\n",
        "\n",
        "\n",
        "def top_k_tracks(tracks: List[Dict[str, Any]], max_num: int\n",
        "                 ) -> List[Dict[str, Any]]:\n",
        "  \"\"\"Select the top-k tracks based on their score.\n",
        "\n",
        "  Args:\n",
        "    tracks (List): A list of dictionaries representing tracks with 'score' as\n",
        "      one of the keys.\n",
        "    max_num (int): The maximum number of tracks to select.\n",
        "\n",
        "  Returns:\n",
        "    List: A list of the top-k tracks with the highest scores.\n",
        "  \"\"\"\n",
        "  sorted_tracks = sorted(tracks, key=lambda d: d['score'], reverse=True)\n",
        "  return sorted_tracks[0:max_num]\n",
        "\n",
        "\n",
        "def run_iou(results: Dict[Any, Any], label_dict: Dict[Any, Any],\n",
        "            max_num_tracks: int = 10) -> Dict[Any, Any]:\n",
        "  \"\"\"Run IoU calculations for ground truth and predicted tracks.\n",
        "\n",
        "  Args:\n",
        "    results (Dict]): A dictionary containing results for different videos,\n",
        "      with video IDs as keys and video results as values.\n",
        "    label_dict (Dict): A dictionary containing label information for different\n",
        "      videos, with video IDs as keys and label data as values.\n",
        "    max_num_tracks (int, optional): The maximum number of tracks to consider.\n",
        "      Defaults to 10.\n",
        "\n",
        "  Returns:\n",
        "    Dict: A dictionary containing video data with IoU scores.\n",
        "  \"\"\"\n",
        "  data = {}\n",
        "  for video_id, video_results in results.items():\n",
        "    video_data = {}\n",
        "    gt_tracks = label_dict[video_id]['object_tracking']\n",
        "    gt_qs = label_dict[video_id]['grounded_question']\n",
        "    pred_answers = video_results['grounded_question']\n",
        "\n",
        "    for q in gt_qs:\n",
        "      q_data = {}\n",
        "      answer_gt_tracks = [gt_tracks[t] for t in q['answers']]\n",
        "      answer_pred_tracks = pred_answers[q['id']]\n",
        "      if len(answer_pred_tracks) > max_num_tracks:\n",
        "        answer_pred_tracks = top_k_tracks(answer_pred_tracks, max_num_tracks)\n",
        "\n",
        "      gt_frame_id_dict = build_gt_ids(gt_tracks, True, q['answers'])\n",
        "      pred_frame_id_dict = build_gt_ids(answer_pred_tracks, False)\n",
        "\n",
        "      q_data['num_gt_ids'] = len(q['answers'])\n",
        "      q_data['num_gt_dets'] = (\n",
        "          sum([len(gt_tracks[t]['bounding_boxes']) for t in q['answers']])\n",
        "      )\n",
        "      q_data['gt_ids'] = [np.array(x) for x in list(gt_frame_id_dict.values())]\n",
        "\n",
        "      q_data['num_tracker_ids'] = len(answer_pred_tracks)\n",
        "      q_data['num_tracker_dets'] = (\n",
        "          sum([len(t['bounding_boxes']) for t in answer_pred_tracks])\n",
        "      )\n",
        "      q_data['tracker_ids'] = (\n",
        "          [np.array(x) for x in list(pred_frame_id_dict.values())]\n",
        "      )\n",
        "\n",
        "      sim_score_dict = {k: [] for k in list(gt_frame_id_dict.keys())}\n",
        "      sim_scores = []\n",
        "      for gt_track in answer_gt_tracks:\n",
        "        track_sim_scores = []\n",
        "        for pred_track in answer_pred_tracks:\n",
        "          start_info = get_start_info(gt_track)\n",
        "          start_idx = start_info['start_idx']\n",
        "          gt_bb = np.array(gt_track['bounding_boxes'])[start_idx:]\n",
        "          gt_fid = gt_track['frame_ids'][start_idx:]\n",
        "\n",
        "          # case where only one box is labelled\n",
        "          if not gt_fid:\n",
        "            continue\n",
        "\n",
        "          pred_bb = np.array(pred_track['bounding_boxes'])\n",
        "          pred_fid = np.array(pred_track['frame_ids'])\n",
        "          # filter predicted trajectory for frame IDs where we have annotations\n",
        "          pred_bb, pred_fid = filter_pred_boxes(pred_bb, pred_fid, gt_fid)\n",
        "          # clip 0 -> 1\n",
        "          pred_bb = np.minimum(np.maximum(pred_bb, 0), 1)\n",
        "          iou = calculate_iou(gt_bb, pred_bb)\n",
        "          track_sim_scores.append(iou)\n",
        "          for frame_iou, frame_id in zip(iou, pred_fid):\n",
        "            sim_score_dict[frame_id].append(frame_iou.item())\n",
        "\n",
        "        sim_scores.append(np.array(track_sim_scores))\n",
        "\n",
        "      if q_data['num_tracker_dets'] == 0 or q_data['num_gt_dets'] == 0:\n",
        "        sim_scores = []\n",
        "      else:\n",
        "        sim_scores = []\n",
        "        for idx, frame_scores in enumerate(sim_score_dict.values()):\n",
        "          sim_scores.append(np.array(frame_scores).reshape(\n",
        "              (len(q_data['gt_ids'][idx]), len(q_data['tracker_ids'][idx])))\n",
        "                           )\n",
        "\n",
        "      q_data['similarity_scores'] = sim_scores\n",
        "      video_data[q['id']] = q_data\n",
        "\n",
        "    data[video_id] = video_data\n",
        "  return data\n",
        "\n",
        "\n",
        "def eval_sequences(evaluator: Any, data: Dict[Any, Any]) -> Dict[str, float]:\n",
        "  \"\"\"Evaluate sequences using the evaluator.\n",
        "\n",
        "  Args:\n",
        "    evaluator (Any): The evaluator object to be used for evaluation.\n",
        "    data (Dict): A dictionary containing the data to be evaluated, with video\n",
        "      IDs as keys and video results as values.\n",
        "\n",
        "  Returns:\n",
        "    Dict: A dictionary containing the average evaluation results for the\n",
        "      sequences, with keys 'HOTA', 'DetA', 'AssA', and 'LocA'.\n",
        "  \"\"\"\n",
        "  hota = []\n",
        "  deta = []\n",
        "  assa = []\n",
        "  loca = []\n",
        "  for video_results in data.values():\n",
        "    for question_results in video_results.values():\n",
        "      res = evaluator.eval_sequence(question_results)\n",
        "      hota.append(np.mean(res['HOTA']))\n",
        "      deta.append(np.mean(res['DetA']))\n",
        "      assa.append(np.mean(res['AssA']))\n",
        "      loca.append(np.mean(res['LocA']))\n",
        "\n",
        "  ave_hota = np.mean(hota)\n",
        "  print('HOTA: ', ave_hota)\n",
        "  ave_deta = np.mean(deta)\n",
        "  print('DetA: ', ave_deta)\n",
        "  ave_assa = np.mean(assa)\n",
        "  print('AssA: ', ave_assa)\n",
        "  ave_loca = np.mean(loca)\n",
        "  print('LocA: ', ave_loca)\n",
        "\n",
        "  return {'HOTA': ave_hota, 'DetA': ave_deta,\n",
        "          'AssA': ave_assa, 'LocA': ave_loca}"
      ],
      "metadata": {
        "id": "P59S1SVy5Odf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if split == 'sample':\n",
        "  label_path = './data/sample.json'\n",
        "\n",
        "elif split == 'valid':\n",
        "  label_path = './data/grounded_question_valid.json'\n",
        "\n",
        "# @title Initialise dataset\n",
        "cfg = {'video_folder_path': './data/videos/',\n",
        "       'task': 'grounded_question',\n",
        "       'split': 'valid'}\n",
        "\n",
        "# init dataset\n",
        "gqa_dataset = PerceptionGQADataset(label_path, **cfg)"
      ],
      "metadata": {
        "id": "6ZGziVpPN25m"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Initialise detector and tracker models\n",
        "DEVICE = 'cuda'\n",
        "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
        "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
        "\n",
        "# The default values correspond to the GQA dataset.\n",
        "mdetr_vqa = mdetr_for_vqa()\n",
        "# But to perform VQA on another dataset, one can simply pass a\n",
        "# different set of heads, e.g.\n",
        "other_vqa_heads = nn.ModuleDict({'head1': nn.Linear(256, 3),\n",
        "                                 'head2': nn.Linear(256, 12)})\n",
        "mdetr_other_vqa_dataset = mdetr_for_vqa(vqa_heads=other_vqa_heads)\n",
        "\n",
        "checkpoint_url = 'https://pytorch.s3.amazonaws.com/models/multimodal/mdetr/gqa_resnet101_checkpoint.pth'\n",
        "checkpoint = torch.hub.load_state_dict_from_url(checkpoint_url,\n",
        "                                                map_location='cpu',\n",
        "                                                check_hash=True)\n",
        "\n",
        "mdetr_vqa.load_state_dict(checkpoint['model_ema'], strict=False)\n",
        "mdetr_vqa.eval().to(DEVICE)\n",
        "\n",
        "img_transform = T.Compose([\n",
        "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
        "\n",
        "# init tracking model\n",
        "object_tracker = ObjectTracker()"
      ],
      "metadata": {
        "id": "NdTACVGWrSOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Get predictions for MDETR + static baseline\n",
        "filter_outputs = True\n",
        "results = {}\n",
        "count_check = 25\n",
        "\n",
        "for cidx, video_item in enumerate(gqa_dataset):\n",
        "  video_id = video_item['metadata']['video_id']\n",
        "\n",
        "  if (cidx%count_check) == 0:\n",
        "    print(cidx, video_id)\n",
        "\n",
        "  img_size = video_item['vqa_frame'].shape[2:]\n",
        "  t_img = img_transform(video_item['vqa_frame'])\n",
        "  gt_tracks = video_item['object_tracking']\n",
        "\n",
        "  video_pred_tracks = {}\n",
        "  for q in video_item['grounded_question']:\n",
        "\n",
        "    combined_frame_id_dict = build_gt_ids(gt_tracks, q['answers'])\n",
        "    tokenized = tokenizer.batch_encode_plus([q['question']], padding='longest',\n",
        "                                            return_tensors='pt')\n",
        "    outputs = mdetr_vqa(t_img.to(DEVICE), tokenized['input_ids'].to(DEVICE))\n",
        "    probs, boxes, labels = postprocess(outputs, q['question'],\n",
        "                                                tokenized, img_size)\n",
        "\n",
        "    qid = 0\n",
        "    question_results = []\n",
        "    for prob, box, label in zip(probs, boxes, labels):\n",
        "      box = rescale_list_boxes(box.detach().cpu().numpy(), img_size)\n",
        "      start_info = {'start_id': 0,\n",
        "                    'start_bounding_box': box,\n",
        "                    'start_idx': 0}\n",
        "\n",
        "      pred_bounding_boxes, pred_frame_ids = (\n",
        "          object_tracker.track_object_in_video(video_item['frames'], start_info)\n",
        "      )\n",
        "      if filter_outputs:\n",
        "        pred_bounding_boxes, pred_frame_ids = (\n",
        "            filter_pred_boxes(pred_bounding_boxes, pred_frame_ids,\n",
        "                              list(combined_frame_id_dict.keys()))\n",
        "        )\n",
        "\n",
        "      pred_track = {}\n",
        "      pred_track['id'] = qid\n",
        "      qid += 1\n",
        "      pred_track['score'] = prob.item()\n",
        "      pred_track['bounding_boxes'] = pred_bounding_boxes.tolist()\n",
        "      pred_track['frame_ids'] = pred_frame_ids.tolist()\n",
        "      question_results.append(pred_track)\n",
        "\n",
        "    video_pred_tracks[q['id']] = question_results\n",
        "\n",
        "  results[video_id] = {'grounded_question': video_pred_tracks}"
      ],
      "metadata": {
        "id": "0Yn588DzlhUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Evaluate predictions\n",
        "label_path = './data/grounded_question_valid.json'\n",
        "label_dict = load_db_json(label_path)\n",
        "data = run_iou(results, label_dict)\n",
        "\n",
        "hota_evaluator = HOTA()\n",
        "eval_results = eval_sequences(hota_evaluator, data)"
      ],
      "metadata": {
        "id": "V82BAOSu7Y1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Serialise example results file\n",
        "# Writing model outputs in the expected competition format. This\n",
        "# JSON file contains answers to all questions in the validation split in the\n",
        "# format:\n",
        "\n",
        "\n",
        "# {'video_8913': {'grounded_question': {5: [{'id': 0, 'score': 0.875480055809021,\n",
        "#     'bounding_boxes': [[0.37, 0.00, 0.83, 0.44],...],\n",
        "#     'frame_ids': [0, 30, 32, 37, 60, 90, 120, 150,...]},...]}}}\n",
        "\n",
        "# This file could be used directly as a submission for the Eval.ai challenge\n",
        "with open(f'{cfg[\"task\"]}_{split}_results.json', 'w') as my_file:\n",
        "  json.dump(results, my_file)"
      ],
      "metadata": {
        "id": "h_dEcM__pV90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Visualisation functions\n",
        "def make_plot(pil_img: Any, scores: List[float], boxes: List[torch.Tensor],\n",
        "              labels: List[str]) -> None:\n",
        "  \"\"\"Plotting utility function.\n",
        "\n",
        "  Args:\n",
        "    pil_img (Any): The PIL image to be plotted (could be of any type,\n",
        "      but should be compatible with 'np.array').\n",
        "    scores (List): List of probabilities for each bounding box.\n",
        "    boxes (List): List of bounding boxes in torch.Tensor format.\n",
        "    labels (List): List of labels for each bounding box.\n",
        "\n",
        "  Returns:\n",
        "    None: The function plots the image with bounding boxes and labels using\n",
        "      matplotlib.\n",
        "  \"\"\"\n",
        "  plt.figure(figsize=(16, 10))\n",
        "  np_image = np.array(pil_img)\n",
        "  ax = plt.gca()\n",
        "  colors = COLORS * 100\n",
        "  assert len(scores) == len(boxes) == len(labels)\n",
        "  for s, box, l, c in zip(scores, boxes, labels, colors):\n",
        "    (xmin, ymin, xmax, ymax) = box.detach().cpu().numpy()\n",
        "    ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
        "                               fill=False, color=c, linewidth=3))\n",
        "    text = f'{l}: {s:0.2f}'\n",
        "    ax.text(xmin, ymin, text, fontsize=15,\n",
        "            bbox=dict(facecolor='white', alpha=0.8))\n",
        "\n",
        "  plt.imshow(np_image)\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def get_colors(num_colors: int) -> Tuple[int, int, int]:\n",
        "  \"\"\"Generate random colormaps for visualizing different objects.\n",
        "\n",
        "  Args:\n",
        "    num_colors (int): The number of colors to generate.\n",
        "\n",
        "  Returns:\n",
        "    Tuple[int, int, int]: A tuple of RGB values representing the\n",
        "      generated colors.\n",
        "  \"\"\"\n",
        "  colors = []\n",
        "  for i in np.arange(0., 360., 360. / num_colors):\n",
        "    hue = i / 360.\n",
        "    lightness = (50 + np.random.rand() * 10) / 100.\n",
        "    saturation = (90 + np.random.rand() * 10) / 100.\n",
        "    color = colorsys.hls_to_rgb(hue, lightness, saturation)\n",
        "    color = (int(color[0] * 255), int(color[1] * 255), int(color[2] * 255))\n",
        "    colors.append(color)\n",
        "  random.seed(0)\n",
        "  random.shuffle(colors)\n",
        "  return colors\n",
        "\n",
        "\n",
        "def display_video(vid_frames: np.array, fps: int = 30):\n",
        "  \"\"\"Create and display temporary video from numpy array frames.\n",
        "\n",
        "  Args:\n",
        "    vid_frames: (np.array): The frames of the video as a\n",
        "    \tnumpy array. Format of frames should be:\n",
        "    \t(num_frames, height, width, channels)\n",
        "    fps (int): Frames per second for the video playback. Default is 30.\n",
        "  \"\"\"\n",
        "  kwargs = {'macro_block_size': None}\n",
        "  imageio.mimwrite('tmp_video_display.mp4',\n",
        "                   vid_frames[:, :, :, ::-1], fps=fps, **kwargs)\n",
        "  display(mvp.ipython_display('tmp_video_display.mp4'))\n",
        "\n",
        "\n",
        "def display_frame(frame: np.array):\n",
        "  \"\"\"Display a frame, converting from RGB to BGR for cv2.\n",
        "\n",
        "  Args:\n",
        "    frame (np.array): The frame to be displayed.\n",
        "  \"\"\"\n",
        "  cv2_imshow(frame)\n",
        "\n",
        "\n",
        "def paint_box(video: np.array, track: Dict[str, Any],\n",
        "              color: Tuple[int, int, int] = (255, 0, 0),\n",
        "              addn_label: str = '') -> np.array:\n",
        "  \"\"\"Paint bounding box and label on video for a given track.\n",
        "\n",
        "  Args:\n",
        "    video (np.array): The video frames as a numpy array.\n",
        "    track (Dict): The track information containing bounding box\n",
        "    and frame information, assumes Perception Test Dataset format.\n",
        "    color (Tuple[int, int, int]): The RGB color values for the bounding box.\n",
        "      Default is red (255, 0, 0).\n",
        "    addn_label (str): Additional label to be added to the track label.\n",
        "      Default is an empty string.\n",
        "\n",
        "  Returns:\n",
        "    np.array: The modified video frames with painted bounding box and\n",
        "      label.\n",
        "  \"\"\"\n",
        "  _, height, width, _ = video.shape\n",
        "  name = str(track['id']) + ' : ' + track['label'] + addn_label\n",
        "  bounding_boxes = np.array(track['bounding_boxes'])\n",
        "\n",
        "  for box, frame_id in zip(bounding_boxes, track['frame_ids']):\n",
        "    frame = np.array(video[frame_id])\n",
        "    x1 = int(round(box[0] * width))\n",
        "    y1 = int(round(box[1] * height))\n",
        "    x2 = int(round(box[2] * width))\n",
        "    y2 = int(round(box[3] * height))\n",
        "    frame = cv2.rectangle(frame, (x1, y1), (x2, y2),\n",
        "                          color=color, thickness=2)\n",
        "    frame = cv2.putText(frame, name, (x1, y1 + 20),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.75, color, 2)\n",
        "    video[frame_id] = frame\n",
        "\n",
        "  return video\n",
        "\n",
        "\n",
        "def paint_boxes(video: np.array, tracks: List[Dict],\n",
        "                colors: Tuple[int, int, int]) -> np.array:\n",
        "  \"\"\"Paint bounding boxes and labels on a video for multiple tracks.\n",
        "\n",
        "  Args:\n",
        "    video (np.array): The video frames as a numpy array.\n",
        "    tracks (List): A list of track information,\n",
        "      where each track contains bounding box and frame information.\n",
        "    colors (Tuple): Tuple containing randomly generated RGB color values.\n",
        "\n",
        "  Returns:\n",
        "    np.array: The modified video frames with painted bounding boxes\n",
        "      and labels.\n",
        "  \"\"\"\n",
        "  for i, track in enumerate(tracks):\n",
        "    video = paint_box(video, track, colors[i])\n",
        "  return video\n",
        "\n",
        "\n",
        "def get_answer_tracks(ex_data: dict, goq_ids: List) -> List[dict]:\n",
        "  \"\"\"Filters and retrieves object tracks based on the given object ids.\n",
        "\n",
        "  Args:\n",
        "    ex_data (dict): The data containing object tracking information.\n",
        "    goq_ids (List): The list of IDs to filter tracks.\n",
        "\n",
        "  Returns:\n",
        "    List[dict]: The filtered tracks matching the goq_ids.\n",
        "  \"\"\"\n",
        "  goq_tracks = []\n",
        "  for track in ex_data['object_tracking']:\n",
        "    if track['id'] in goq_ids:\n",
        "      goq_tracks.append(track)\n",
        "  return goq_tracks"
      ],
      "metadata": {
        "id": "dafGHrzlPnBL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Ground truth annotations visualised\n",
        "colors = get_colors(num_colors=100)\n",
        "\n",
        "# sample annotations and videos to visualise the annotations later\n",
        "sample_annot_url = 'https://storage.googleapis.com/dm-perception-test/zip_data/sample_annotations.zip'\n",
        "download_and_unzip(sample_annot_url, data_path)\n",
        "\n",
        "sample_videos_url = 'https://storage.googleapis.com/dm-perception-test/zip_data/sample_videos.zip'\n",
        "download_and_unzip(sample_videos_url, data_path)\n",
        "\n",
        "# get sample video info to showcase the annotations\n",
        "sample_db_path = './data/sample.json'\n",
        "sample_db_dict = load_db_json(sample_db_path)\n",
        "video_id = list(sample_db_dict.keys())[7]\n",
        "example_data = sample_db_dict[video_id]\n",
        "\n",
        "if example_data['grounded_question']:\n",
        "  question = example_data['grounded_question'][0]\n",
        "  print('---------------------------------')\n",
        "  print('Question: ', question['question'])\n",
        "  print('Answer IDs: ', question['answers'])\n",
        "  print('Question info: ')\n",
        "  print('Reasoning: ', question['reasoning'])\n",
        "  print('area: ', question['area'])\n",
        "  print('---------------------------------')\n",
        "\n",
        "  frames = get_video_frames(example_data, cfg['video_folder_path'])\n",
        "  answer_tracks = get_answer_tracks(example_data, question['answers'])\n",
        "  frames = paint_boxes(frames, answer_tracks, colors)\n",
        "\n",
        "  annotated_frames = []\n",
        "  for frame_idx in answer_tracks[0]['frame_ids']:\n",
        "    annotated_frames.append(frames[frame_idx])\n",
        "\n",
        "  annotated_frames = np.array(annotated_frames)\n",
        "  display_video(annotated_frames, 1)\n",
        "  del frames"
      ],
      "metadata": {
        "id": "hPJkHKWPEvff"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}